{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils import data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from functions import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ------------------- label conversion tools ------------------ ##\n",
    "def labels2cat(label_encoder, list):\n",
    "    return label_encoder.transform(list)\n",
    "\n",
    "def labels2onehot(OneHotEncoder, label_encoder, list):\n",
    "    return OneHotEncoder.transform(label_encoder.transform(list).reshape(-1, 1)).toarray()\n",
    "\n",
    "def onehot2labels(label_encoder, y_onehot):\n",
    "    return label_encoder.inverse_transform(np.where(y_onehot == 1)[1]).tolist()\n",
    "\n",
    "def cat2labels(label_encoder, y_cat):\n",
    "    return label_encoder.inverse_transform(y_cat).tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for CRNN\n",
    "class Dataset_CRNN(data.Dataset):\n",
    "    \"Characterizes a dataset for PyTorch\"\n",
    "    def __init__(self, data_path, folders, labels, frames, transform=None):\n",
    "        \"Initialization\"\n",
    "        self.data_path = data_path\n",
    "        self.labels = labels\n",
    "        self.folders = folders\n",
    "        self.transform = transform\n",
    "        self.frames = frames\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Denotes the total number of samples\"\n",
    "        return len(self.folders)\n",
    "\n",
    "    def read_images(self, path, selected_folder, use_transform):\n",
    "        X = []\n",
    "        for i in self.frames:\n",
    "            image = Image.open(os.path.join(path, selected_folder, '{:05d}.jpg'.format(i)))\n",
    "\n",
    "            if use_transform is not None:\n",
    "                image = use_transform(image)\n",
    "\n",
    "            X.append(image)\n",
    "        X = torch.stack(X, dim=0)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"Generates one sample of data\"\n",
    "        # Select sample\n",
    "        folder = self.folders[index]\n",
    "\n",
    "        # Load data\n",
    "        X = self.read_images(self.data_path, folder, self.transform)     # (input) spatial images\n",
    "        y = torch.LongTensor([self.labels[index]])                  # (labels) LongTensor are for int64 instead of FloatTensor\n",
    "\n",
    "        # print(X.shape)\n",
    "        return X, y\n",
    "\n",
    "## ---------------------- end of Dataloaders ---------------------- ##\n",
    "\n",
    "\n",
    "\n",
    "## -------------------- (reload) model prediction ---------------------- ##\n",
    "def Conv3d_final_prediction(model, device, loader):\n",
    "    model.eval()\n",
    "\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (X, y) in enumerate(tqdm(loader)):\n",
    "            # distribute data to device\n",
    "            X = X.to(device)\n",
    "            output = model(X)\n",
    "            y_pred = output.max(1, keepdim=True)[1]  # location of max log-probability as prediction\n",
    "            all_y_pred.extend(y_pred.cpu().data.squeeze().numpy().tolist())\n",
    "\n",
    "    return all_y_pred\n",
    "\n",
    "\n",
    "def CRNN_final_prediction(model, device, loader):\n",
    "    cnn_encoder, rnn_decoder = model\n",
    "    cnn_encoder.eval()\n",
    "    rnn_decoder.eval()\n",
    "\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (X, y) in enumerate(tqdm(loader)):\n",
    "            # distribute data to device\n",
    "            X = X.to(device)\n",
    "            output = rnn_decoder(cnn_encoder(X))\n",
    "            y_pred = output.max(1, keepdim=True)[1]  # location of max log-probability as prediction\n",
    "            all_y_pred.extend(y_pred.cpu().data.squeeze().numpy().tolist())\n",
    "\n",
    "    return all_y_pred\n",
    "\n",
    "## -------------------- end of model prediction ---------------------- ##\n",
    "\n",
    "\n",
    "\n",
    "## ------------------------ 3D CNN module ---------------------- ##\n",
    "def conv3D_output_size(img_size, padding, kernel_size, stride):\n",
    "    # compute output shape of conv3D\n",
    "    outshape = (np.floor((img_size[0] + 2 * padding[0] - (kernel_size[0] - 1) - 1) / stride[0] + 1).astype(int),\n",
    "                np.floor((img_size[1] + 2 * padding[1] - (kernel_size[1] - 1) - 1) / stride[1] + 1).astype(int),\n",
    "                np.floor((img_size[2] + 2 * padding[2] - (kernel_size[2] - 1) - 1) / stride[2] + 1).astype(int))\n",
    "    return outshape\n",
    "\n",
    "class CNN3D(nn.Module):\n",
    "    def __init__(self, t_dim=120, img_x=90, img_y=120, drop_p=0.2, fc_hidden1=256, fc_hidden2=128, num_classes=50):\n",
    "        super(CNN3D, self).__init__()\n",
    "\n",
    "        # set video dimension\n",
    "        self.t_dim = t_dim\n",
    "        self.img_x = img_x\n",
    "        self.img_y = img_y\n",
    "        # fully connected layer hidden nodes\n",
    "        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2\n",
    "        self.drop_p = drop_p\n",
    "        self.num_classes = num_classes\n",
    "        self.ch1, self.ch2 = 32, 48\n",
    "        self.k1, self.k2 = (5, 5, 5), (3, 3, 3)  # 3d kernel size\n",
    "        self.s1, self.s2 = (2, 2, 2), (2, 2, 2)  # 3d strides\n",
    "        self.pd1, self.pd2 = (0, 0, 0), (0, 0, 0)  # 3d padding\n",
    "\n",
    "        # compute conv1 & conv2 output shape\n",
    "        self.conv1_outshape = conv3D_output_size((self.t_dim, self.img_x, self.img_y), self.pd1, self.k1, self.s1)\n",
    "        self.conv2_outshape = conv3D_output_size(self.conv1_outshape, self.pd2, self.k2, self.s2)\n",
    "\n",
    "        self.conv1 = nn.Conv3d(in_channels=1, out_channels=self.ch1, kernel_size=self.k1, stride=self.s1,\n",
    "                               padding=self.pd1)\n",
    "        self.bn1 = nn.BatchNorm3d(self.ch1)\n",
    "        self.conv2 = nn.Conv3d(in_channels=self.ch1, out_channels=self.ch2, kernel_size=self.k2, stride=self.s2,\n",
    "                               padding=self.pd2)\n",
    "        self.bn2 = nn.BatchNorm3d(self.ch2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.drop = nn.Dropout3d(self.drop_p)\n",
    "        self.pool = nn.MaxPool3d(2)\n",
    "        self.fc1 = nn.Linear(self.ch2 * self.conv2_outshape[0] * self.conv2_outshape[1] * self.conv2_outshape[2],\n",
    "                             self.fc_hidden1)  # fully connected hidden layer\n",
    "        self.fc2 = nn.Linear(self.fc_hidden1, self.fc_hidden2)\n",
    "        self.fc3 = nn.Linear(self.fc_hidden2, self.num_classes)  # fully connected layer, output = multi-classes\n",
    "\n",
    "    def forward(self, x_3d):\n",
    "        # Conv 1\n",
    "        x = self.conv1(x_3d)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        # Conv 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        # FC 1 and 2\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=self.drop_p, training=self.training)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "## --------------------- end of 3D CNN module ---------------- ##\n",
    "\n",
    "\n",
    "\n",
    "## ------------------------ CRNN module ---------------------- ##\n",
    "\n",
    "def conv2D_output_size(img_size, padding, kernel_size, stride):\n",
    "    # compute output shape of conv2D\n",
    "    outshape = (np.floor((img_size[0] + 2 * padding[0] - (kernel_size[0] - 1) - 1) / stride[0] + 1).astype(int),\n",
    "                np.floor((img_size[1] + 2 * padding[1] - (kernel_size[1] - 1) - 1) / stride[1] + 1).astype(int))\n",
    "    return outshape\n",
    "\n",
    "\n",
    "# 2D CNN encoder train from scratch (no transfer learning)\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, img_x=90, img_y=120, fc_hidden1=512, fc_hidden2=512, drop_p=0.3, CNN_embed_dim=300):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "\n",
    "        self.img_x = img_x\n",
    "        self.img_y = img_y\n",
    "        self.CNN_embed_dim = CNN_embed_dim\n",
    "\n",
    "        # CNN architechtures\n",
    "        self.ch1, self.ch2, self.ch3, self.ch4 = 32, 64, 128, 256\n",
    "        self.k1, self.k2, self.k3, self.k4 = (5, 5), (3, 3), (3, 3), (3, 3)      # 2d kernal size\n",
    "        self.s1, self.s2, self.s3, self.s4 = (2, 2), (2, 2), (2, 2), (2, 2)      # 2d strides\n",
    "        self.pd1, self.pd2, self.pd3, self.pd4 = (0, 0), (0, 0), (0, 0), (0, 0)  # 2d padding\n",
    "\n",
    "        # conv2D output shapes\n",
    "        self.conv1_outshape = conv2D_output_size((self.img_x, self.img_y), self.pd1, self.k1, self.s1)  # Conv1 output shape\n",
    "        self.conv2_outshape = conv2D_output_size(self.conv1_outshape, self.pd2, self.k2, self.s2)\n",
    "        self.conv3_outshape = conv2D_output_size(self.conv2_outshape, self.pd3, self.k3, self.s3)\n",
    "        self.conv4_outshape = conv2D_output_size(self.conv3_outshape, self.pd4, self.k4, self.s4)\n",
    "\n",
    "        # fully connected layer hidden nodes\n",
    "        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2\n",
    "        self.drop_p = drop_p\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=self.ch1, kernel_size=self.k1, stride=self.s1, padding=self.pd1),\n",
    "            nn.BatchNorm2d(self.ch1, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),                      \n",
    "            # nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.ch1, out_channels=self.ch2, kernel_size=self.k2, stride=self.s2, padding=self.pd2),\n",
    "            nn.BatchNorm2d(self.ch2, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.ch2, out_channels=self.ch3, kernel_size=self.k3, stride=self.s3, padding=self.pd3),\n",
    "            nn.BatchNorm2d(self.ch3, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.ch3, out_channels=self.ch4, kernel_size=self.k4, stride=self.s4, padding=self.pd4),\n",
    "            nn.BatchNorm2d(self.ch4, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.drop = nn.Dropout2d(self.drop_p)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(self.ch4 * self.conv4_outshape[0] * self.conv4_outshape[1], self.fc_hidden1)   # fully connected layer, output k classes\n",
    "        self.fc2 = nn.Linear(self.fc_hidden1, self.fc_hidden2)\n",
    "        self.fc3 = nn.Linear(self.fc_hidden2, self.CNN_embed_dim)   # output = CNN embedding latent variables\n",
    "\n",
    "    def forward(self, x_3d):\n",
    "        cnn_embed_seq = []\n",
    "        for t in range(x_3d.size(1)):\n",
    "            # CNNs\n",
    "            x = self.conv1(x_3d[:, t, :, :, :])\n",
    "            x = self.conv2(x)\n",
    "            x = self.conv3(x)\n",
    "            x = self.conv4(x)\n",
    "            x = x.view(x.size(0), -1)           # flatten the output of conv\n",
    "\n",
    "            # FC layers\n",
    "            x = F.relu(self.fc1(x))\n",
    "            # x = F.dropout(x, p=self.drop_p, training=self.training)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = F.dropout(x, p=self.drop_p, training=self.training)\n",
    "            x = self.fc3(x)\n",
    "            cnn_embed_seq.append(x)\n",
    "\n",
    "        # swap time and sample dim such that (sample dim, time dim, CNN latent dim)\n",
    "        cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1)\n",
    "        # cnn_embed_seq: shape=(batch, time_step, input_size)\n",
    "\n",
    "        return cnn_embed_seq\n",
    "\n",
    "\n",
    "# 2D CNN encoder using ResNet-152 pretrained\n",
    "class ResCNNEncoder(nn.Module):\n",
    "    def __init__(self, fc_hidden1=512, fc_hidden2=512, drop_p=0.3, CNN_embed_dim=300):\n",
    "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "        super(ResCNNEncoder, self).__init__()\n",
    "\n",
    "        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2\n",
    "        self.drop_p = drop_p\n",
    "\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.fc1 = nn.Linear(resnet.fc.in_features, fc_hidden1)\n",
    "        self.bn1 = nn.BatchNorm1d(fc_hidden1, momentum=0.01)\n",
    "        self.fc2 = nn.Linear(fc_hidden1, fc_hidden2)\n",
    "        self.bn2 = nn.BatchNorm1d(fc_hidden2, momentum=0.01)\n",
    "        self.fc3 = nn.Linear(fc_hidden2, CNN_embed_dim)\n",
    "        \n",
    "    def forward(self, x_3d):\n",
    "        cnn_embed_seq = []\n",
    "        for t in range(x_3d.size(1)):\n",
    "            # ResNet CNN\n",
    "            with torch.no_grad():\n",
    "                x = self.resnet(x_3d[:, t, :, :, :])  # ResNet\n",
    "                x = x.view(x.size(0), -1)             # flatten output of conv\n",
    "\n",
    "            # FC layers\n",
    "            x = self.bn1(self.fc1(x))\n",
    "            x = F.relu(x)\n",
    "            x = self.bn2(self.fc2(x))\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.drop_p, training=self.training)\n",
    "            x = self.fc3(x)\n",
    "\n",
    "            cnn_embed_seq.append(x)\n",
    "\n",
    "        # swap time and sample dim such that (sample dim, time dim, CNN latent dim)\n",
    "        cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1)\n",
    "        # cnn_embed_seq: shape=(batch, time_step, input_size)\n",
    "\n",
    "        return cnn_embed_seq\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, CNN_embed_dim=300, h_RNN_layers=3, h_RNN=256, h_FC_dim=128, drop_p=0.3, num_classes=50):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        self.RNN_input_size = CNN_embed_dim\n",
    "        self.h_RNN_layers = h_RNN_layers   # RNN hidden layers\n",
    "        self.h_RNN = h_RNN                 # RNN hidden nodes\n",
    "        self.h_FC_dim = h_FC_dim\n",
    "        self.drop_p = drop_p\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.LSTM = nn.LSTM(\n",
    "            input_size=self.RNN_input_size,\n",
    "            hidden_size=self.h_RNN,        \n",
    "            num_layers=h_RNN_layers,       \n",
    "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(self.h_RNN, self.h_FC_dim)\n",
    "        self.fc2 = nn.Linear(self.h_FC_dim, self.num_classes)\n",
    "\n",
    "    def forward(self, x_RNN):\n",
    "        \n",
    "        self.LSTM.flatten_parameters()\n",
    "        RNN_out, (h_n, h_c) = self.LSTM(x_RNN, None)  \n",
    "        \"\"\" h_n shape (n_layers, batch, hidden_size), h_c shape (n_layers, batch, hidden_size) \"\"\" \n",
    "        \"\"\" None represents zero initial hidden state. RNN_out has shape=(batch, time_step, output_size) \"\"\"\n",
    "\n",
    "        # FC layers\n",
    "        x = self.fc1(RNN_out[:, -1, :])   # choose RNN_out at the last time step\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.drop_p, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "## ---------------------- end of CRNN module ---------------------- ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    # set model as training mode\n",
    "    cnn_encoder, rnn_decoder = model\n",
    "    cnn_encoder.train()\n",
    "    rnn_decoder.train()\n",
    "\n",
    "    losses = []\n",
    "    scores = []\n",
    "    N_count = 0   # counting total trained sample in one epoch\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        # distribute data to device\n",
    "        X, y = X.to(device), y.to(device).view(-1, )\n",
    "\n",
    "        N_count += X.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = rnn_decoder(cnn_encoder(X))   # output has dim = (batch, number of classes)\n",
    "\n",
    "        loss = F.cross_entropy(output, y)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # to compute accuracy\n",
    "        y_pred = torch.max(output, 1)[1]  # y_pred != output\n",
    "        step_score = accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy())\n",
    "        scores.append(step_score)         # computed on CPU\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # show information\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accu: {:.2f}%'.format(\n",
    "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item(), 100 * step_score))\n",
    "\n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, device, optimizer, test_loader, epoch):\n",
    "    # set model as testing mode\n",
    "    cnn_encoder, rnn_decoder = model\n",
    "    cnn_encoder.eval()\n",
    "    rnn_decoder.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            # distribute data to device\n",
    "            X, y = X.to(device), y.to(device).view(-1, )\n",
    "\n",
    "            output = rnn_decoder(cnn_encoder(X))\n",
    "\n",
    "            loss = F.cross_entropy(output, y, reduction='sum')\n",
    "            test_loss += loss.item()                 # sum up batch loss\n",
    "            y_pred = output.max(1, keepdim=True)[1]  # (y_pred != output) get the index of the max log-probability\n",
    "\n",
    "            # collect all y and y_pred in all batches\n",
    "            all_y.extend(y)\n",
    "            all_y_pred.extend(y_pred)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # compute accuracy\n",
    "    all_y = torch.stack(all_y, dim=0)\n",
    "    all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "    test_score = accuracy_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy())\n",
    "\n",
    "    # show information\n",
    "    print('\\nTest set ({:d} samples): Average loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(len(all_y), test_loss, 100* test_score))\n",
    "\n",
    "    # save Pytorch models of best record\n",
    "    torch.save(cnn_encoder.state_dict(), os.path.join(save_model_path, 'cnn_encoder_epoch{}.pth'.format(epoch + 1)))  # save spatial_encoder\n",
    "    torch.save(rnn_decoder.state_dict(), os.path.join(save_model_path, 'rnn_decoder_epoch{}.pth'.format(epoch + 1)))  # save motion_encoder\n",
    "    torch.save(optimizer.state_dict(), os.path.join(save_model_path, 'optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
    "    print(\"Epoch {} model saved!\".format(epoch + 1))\n",
    "\n",
    "    return test_loss, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path\n",
    "data_path = \"F:/STAT453-Project/sub_dataset\"    # define UCF-101 RGB data path\n",
    "\n",
    "# action_name_path = './UCF101actions.pkl'\n",
    "save_model_path = \"./CRNN_ckpt/\"\n",
    "\n",
    "# EncoderCNN architecture\n",
    "CNN_fc_hidden1, CNN_fc_hidden2 = 1024, 768\n",
    "CNN_embed_dim = 512      # latent dim extracted by 2D CNN\n",
    "img_x, img_y = 100, 176  # resize video 2d frame size\n",
    "dropout_p = 0.0          # dropout probability\n",
    "\n",
    "# DecoderRNN architecture\n",
    "RNN_hidden_layers = 3\n",
    "RNN_hidden_nodes = 512\n",
    "RNN_FC_dim = 256\n",
    "\n",
    "# training parameters\n",
    "k = 10             # number of target category\n",
    "epochs = 120        # training epochs\n",
    "batch_size = 30  \n",
    "learning_rate = 1e-4\n",
    "log_interval = 10   # interval for displaying training info\n",
    "\n",
    "# Select which frame to begin & end in videos\n",
    "begin_frame, end_frame, skip_frame = 1, 10, 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Detect devices\n",
    "    use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")   # use CPU or GPU\n",
    "\n",
    "    # Data loading parameters\n",
    "    params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 0, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "    # load actions names\n",
    "    action_names = pd.read_csv('labels.csv')['Label'].tolist()\n",
    "\n",
    "    # convert labels -> category\n",
    "    le = LabelEncoder()\n",
    "    le.fit(action_names)\n",
    "\n",
    "    # show how many classes there are\n",
    "    list(le.classes_)\n",
    "\n",
    "    # convert category -> 1-hot\n",
    "    action_category = le.transform(action_names).reshape(-1, 1)\n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(action_category)\n",
    "\n",
    "    # # example\n",
    "    # y = ['HorseRace', 'YoYo', 'WalkingWithDog']\n",
    "    # y_onehot = labels2onehot(enc, le, y)\n",
    "    # y2 = onehot2labels(le, y_onehot)\n",
    "\n",
    "\n",
    "#     # TODO read train.csv\n",
    "#     actions = []\n",
    "#     fnames = os.listdir(data_path)\n",
    "\n",
    "#     all_names = []\n",
    "#     for f in fnames:\n",
    "#         loc1 = f.find('v_')\n",
    "#         loc2 = f.find('_g')\n",
    "#         actions.append(f[(loc1 + 2): loc2])\n",
    "\n",
    "#         all_names.append(f)\n",
    "\n",
    "\n",
    "#     # list all data files\n",
    "#     all_X_list = all_names                  # all video file names\n",
    "#     all_y_list = labels2cat(le, actions)    # all video labels\n",
    "\n",
    "#     # train, test split\n",
    "#     # TODO\n",
    "#     train_list, test_list, train_label, test_label = train_test_split(all_X_list, all_y_list, test_size=0.25, random_state=42)\n",
    "    \n",
    "    train_df = pd.read_csv('train_set.csv')\n",
    "    \n",
    "    train_list = list(map(str, train_df['Index'].tolist()))\n",
    "    train_label = labels2cat(le, train_df['Label'].tolist())\n",
    "    \n",
    "    test_df = pd.read_csv('val_set.csv')\n",
    "    test_list = list(map(str, test_df['Index'].tolist()))\n",
    "    test_label = labels2cat(le, test_df['Label'].tolist())\n",
    "\n",
    "    transform = transforms.Compose([transforms.Resize([img_x, img_y]),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "    selected_frames = np.arange(begin_frame, end_frame, skip_frame).tolist()\n",
    "\n",
    "    # ?\n",
    "    \n",
    "    train_set, valid_set = Dataset_CRNN(os.path.join(data_path, \"train\"), train_list, train_label, selected_frames, transform=transform), \\\n",
    "                           Dataset_CRNN(os.path.join(data_path, \"val\"), test_list, test_label, selected_frames, transform=transform)\n",
    "\n",
    "    train_loader = data.DataLoader(train_set, **params)\n",
    "    valid_loader = data.DataLoader(valid_set, **params)\n",
    "\n",
    "    # Create model\n",
    "    cnn_encoder = EncoderCNN(img_x=img_x, img_y=img_y, fc_hidden1=CNN_fc_hidden1, fc_hidden2=CNN_fc_hidden2,\n",
    "                             drop_p=dropout_p, CNN_embed_dim=CNN_embed_dim).to(device)\n",
    "\n",
    "    rnn_decoder = DecoderRNN(CNN_embed_dim=CNN_embed_dim, h_RNN_layers=RNN_hidden_layers, h_RNN=RNN_hidden_nodes, \n",
    "                             h_FC_dim=RNN_FC_dim, drop_p=dropout_p, num_classes=k).to(device)\n",
    "\n",
    "#     # Parallelize model to multiple GPUs\n",
    "#     if torch.cuda.device_count() > 1:\n",
    "#         print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#         cnn_encoder = nn.DataParallel(cnn_encoder)\n",
    "#         rnn_decoder = nn.DataParallel(rnn_decoder)\n",
    "\n",
    "    crnn_params = list(cnn_encoder.parameters()) + list(rnn_decoder.parameters())\n",
    "    optimizer = torch.optim.Adam(crnn_params, lr=learning_rate)\n",
    "\n",
    "\n",
    "    # record training process\n",
    "    epoch_train_losses = []\n",
    "    epoch_train_scores = []\n",
    "    epoch_test_losses = []\n",
    "    epoch_test_scores = []\n",
    "\n",
    "    # start training\n",
    "    for epoch in range(epochs):\n",
    "        # train, test model\n",
    "        train_losses, train_scores = train(log_interval, [cnn_encoder, rnn_decoder], device, train_loader, optimizer, epoch)\n",
    "        epoch_test_loss, epoch_test_score = validation([cnn_encoder, rnn_decoder], device, optimizer, valid_loader, epoch)\n",
    "\n",
    "        # save results\n",
    "        epoch_train_losses.append(train_losses)\n",
    "        epoch_train_scores.append(train_scores)\n",
    "        epoch_test_losses.append(epoch_test_loss)\n",
    "        epoch_test_scores.append(epoch_test_score)\n",
    "\n",
    "        # save all train test results\n",
    "        A = np.array(epoch_train_losses)\n",
    "        B = np.array(epoch_train_scores)\n",
    "        C = np.array(epoch_test_losses)\n",
    "        D = np.array(epoch_test_scores)\n",
    "        np.save('./CRNN_epoch_training_losses.npy', A)\n",
    "        np.save('./CRNN_epoch_training_scores.npy', B)\n",
    "        np.save('./CRNN_epoch_test_loss.npy', C)\n",
    "        np.save('./CRNN_epoch_test_score.npy', D)\n",
    "\n",
    "    # plot\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(np.arange(1, epochs + 1), A[:, -1])  # train loss (on epoch end)\n",
    "    plt.plot(np.arange(1, epochs + 1), C)         #  test loss (on epoch end)\n",
    "    plt.title(\"model loss\")\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['train', 'test'], loc=\"upper left\")\n",
    "    # 2nd figure\n",
    "    plt.subplot(122)\n",
    "    plt.plot(np.arange(1, epochs + 1), B[:, -1])  # train accuracy (on epoch end)\n",
    "    plt.plot(np.arange(1, epochs + 1), D)         #  test accuracy (on epoch end)\n",
    "    plt.title(\"training scores\")\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(['train', 'test'], loc=\"upper left\")\n",
    "    title = \"./fig_sub_Jester_CRNN.png\"\n",
    "    plt.savefig(title, dpi=600)\n",
    "    # plt.close(fig)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [300/11291 (3%)]\tLoss: 2.299834, Accu: 13.33%\n",
      "Train Epoch: 1 [600/11291 (5%)]\tLoss: 2.300450, Accu: 3.33%\n",
      "Train Epoch: 1 [900/11291 (8%)]\tLoss: 2.306102, Accu: 6.67%\n",
      "Train Epoch: 1 [1200/11291 (11%)]\tLoss: 2.299658, Accu: 3.33%\n",
      "Train Epoch: 1 [1500/11291 (13%)]\tLoss: 2.306126, Accu: 13.33%\n",
      "Train Epoch: 1 [1800/11291 (16%)]\tLoss: 2.305005, Accu: 3.33%\n",
      "Train Epoch: 1 [2100/11291 (19%)]\tLoss: 2.298254, Accu: 16.67%\n",
      "Train Epoch: 1 [2400/11291 (21%)]\tLoss: 2.295262, Accu: 6.67%\n",
      "Train Epoch: 1 [2700/11291 (24%)]\tLoss: 2.303683, Accu: 13.33%\n",
      "Train Epoch: 1 [3000/11291 (27%)]\tLoss: 2.301844, Accu: 6.67%\n",
      "Train Epoch: 1 [3300/11291 (29%)]\tLoss: 2.303556, Accu: 3.33%\n",
      "Train Epoch: 1 [3600/11291 (32%)]\tLoss: 2.294472, Accu: 13.33%\n",
      "Train Epoch: 1 [3900/11291 (34%)]\tLoss: 2.306377, Accu: 0.00%\n",
      "Train Epoch: 1 [4200/11291 (37%)]\tLoss: 2.288962, Accu: 16.67%\n",
      "Train Epoch: 1 [4500/11291 (40%)]\tLoss: 2.302506, Accu: 10.00%\n",
      "Train Epoch: 1 [4800/11291 (42%)]\tLoss: 2.301205, Accu: 0.00%\n",
      "Train Epoch: 1 [5100/11291 (45%)]\tLoss: 2.304603, Accu: 6.67%\n",
      "Train Epoch: 1 [5400/11291 (48%)]\tLoss: 2.285087, Accu: 26.67%\n",
      "Train Epoch: 1 [5700/11291 (50%)]\tLoss: 2.306568, Accu: 6.67%\n",
      "Train Epoch: 1 [6000/11291 (53%)]\tLoss: 2.304626, Accu: 6.67%\n",
      "Train Epoch: 1 [6300/11291 (56%)]\tLoss: 2.296762, Accu: 10.00%\n",
      "Train Epoch: 1 [6600/11291 (58%)]\tLoss: 2.291843, Accu: 6.67%\n",
      "Train Epoch: 1 [6900/11291 (61%)]\tLoss: 2.319130, Accu: 6.67%\n",
      "Train Epoch: 1 [7200/11291 (64%)]\tLoss: 2.292768, Accu: 6.67%\n",
      "Train Epoch: 1 [7500/11291 (66%)]\tLoss: 2.306750, Accu: 10.00%\n",
      "Train Epoch: 1 [7800/11291 (69%)]\tLoss: 2.305762, Accu: 6.67%\n",
      "Train Epoch: 1 [8100/11291 (72%)]\tLoss: 2.300900, Accu: 10.00%\n",
      "Train Epoch: 1 [8400/11291 (74%)]\tLoss: 2.311302, Accu: 6.67%\n",
      "Train Epoch: 1 [8700/11291 (77%)]\tLoss: 2.304883, Accu: 3.33%\n",
      "Train Epoch: 1 [9000/11291 (80%)]\tLoss: 2.311933, Accu: 0.00%\n",
      "Train Epoch: 1 [9300/11291 (82%)]\tLoss: 2.296614, Accu: 20.00%\n",
      "Train Epoch: 1 [9600/11291 (85%)]\tLoss: 2.308595, Accu: 10.00%\n",
      "Train Epoch: 1 [9900/11291 (88%)]\tLoss: 2.305863, Accu: 10.00%\n",
      "Train Epoch: 1 [10200/11291 (90%)]\tLoss: 2.300636, Accu: 10.00%\n",
      "Train Epoch: 1 [10500/11291 (93%)]\tLoss: 2.294503, Accu: 16.67%\n",
      "Train Epoch: 1 [10800/11291 (95%)]\tLoss: 2.324686, Accu: 0.00%\n",
      "Train Epoch: 1 [11100/11291 (98%)]\tLoss: 2.307182, Accu: 3.33%\n",
      "\n",
      "Test set (1411 samples): Average loss: 2.3006, Accuracy: 10.42%\n",
      "\n",
      "Epoch 1 model saved!\n",
      "Train Epoch: 2 [300/11291 (3%)]\tLoss: 2.292927, Accu: 10.00%\n",
      "Train Epoch: 2 [600/11291 (5%)]\tLoss: 2.314472, Accu: 6.67%\n",
      "Train Epoch: 2 [900/11291 (8%)]\tLoss: 2.305186, Accu: 6.67%\n",
      "Train Epoch: 2 [1200/11291 (11%)]\tLoss: 2.299326, Accu: 3.33%\n",
      "Train Epoch: 2 [1500/11291 (13%)]\tLoss: 2.296351, Accu: 13.33%\n",
      "Train Epoch: 2 [1800/11291 (16%)]\tLoss: 2.286693, Accu: 23.33%\n",
      "Train Epoch: 2 [2100/11291 (19%)]\tLoss: 2.298861, Accu: 13.33%\n",
      "Train Epoch: 2 [2400/11291 (21%)]\tLoss: 2.293917, Accu: 10.00%\n",
      "Train Epoch: 2 [2700/11291 (24%)]\tLoss: 2.295422, Accu: 3.33%\n",
      "Train Epoch: 2 [3000/11291 (27%)]\tLoss: 2.297521, Accu: 13.33%\n",
      "Train Epoch: 2 [3300/11291 (29%)]\tLoss: 2.319547, Accu: 6.67%\n",
      "Train Epoch: 2 [3600/11291 (32%)]\tLoss: 2.332073, Accu: 3.33%\n",
      "Train Epoch: 2 [3900/11291 (34%)]\tLoss: 2.317598, Accu: 6.67%\n",
      "Train Epoch: 2 [4200/11291 (37%)]\tLoss: 2.302361, Accu: 13.33%\n",
      "Train Epoch: 2 [4500/11291 (40%)]\tLoss: 2.314167, Accu: 3.33%\n",
      "Train Epoch: 2 [4800/11291 (42%)]\tLoss: 2.301245, Accu: 13.33%\n",
      "Train Epoch: 2 [5100/11291 (45%)]\tLoss: 2.305997, Accu: 3.33%\n",
      "Train Epoch: 2 [5400/11291 (48%)]\tLoss: 2.300324, Accu: 10.00%\n",
      "Train Epoch: 2 [5700/11291 (50%)]\tLoss: 2.296216, Accu: 13.33%\n",
      "Train Epoch: 2 [6000/11291 (53%)]\tLoss: 2.334194, Accu: 6.67%\n",
      "Train Epoch: 2 [6300/11291 (56%)]\tLoss: 2.294606, Accu: 3.33%\n",
      "Train Epoch: 2 [6600/11291 (58%)]\tLoss: 2.287382, Accu: 13.33%\n",
      "Train Epoch: 2 [6900/11291 (61%)]\tLoss: 2.297396, Accu: 13.33%\n",
      "Train Epoch: 2 [7200/11291 (64%)]\tLoss: 2.293473, Accu: 13.33%\n",
      "Train Epoch: 2 [7500/11291 (66%)]\tLoss: 2.287408, Accu: 10.00%\n",
      "Train Epoch: 2 [7800/11291 (69%)]\tLoss: 2.305061, Accu: 20.00%\n",
      "Train Epoch: 2 [8100/11291 (72%)]\tLoss: 2.298400, Accu: 6.67%\n",
      "Train Epoch: 2 [8400/11291 (74%)]\tLoss: 2.305777, Accu: 10.00%\n",
      "Train Epoch: 2 [8700/11291 (77%)]\tLoss: 2.306288, Accu: 20.00%\n",
      "Train Epoch: 2 [9000/11291 (80%)]\tLoss: 2.310824, Accu: 16.67%\n",
      "Train Epoch: 2 [9300/11291 (82%)]\tLoss: 2.294887, Accu: 13.33%\n",
      "Train Epoch: 2 [9600/11291 (85%)]\tLoss: 2.290141, Accu: 3.33%\n",
      "Train Epoch: 2 [9900/11291 (88%)]\tLoss: 2.298173, Accu: 6.67%\n",
      "Train Epoch: 2 [10200/11291 (90%)]\tLoss: 2.287593, Accu: 13.33%\n",
      "Train Epoch: 2 [10500/11291 (93%)]\tLoss: 2.303259, Accu: 13.33%\n",
      "Train Epoch: 2 [10800/11291 (95%)]\tLoss: 2.325608, Accu: 0.00%\n",
      "Train Epoch: 2 [11100/11291 (98%)]\tLoss: 2.279214, Accu: 6.67%\n",
      "\n",
      "Test set (1411 samples): Average loss: 2.2998, Accuracy: 11.13%\n",
      "\n",
      "Epoch 2 model saved!\n",
      "Train Epoch: 3 [300/11291 (3%)]\tLoss: 2.293607, Accu: 6.67%\n",
      "Train Epoch: 3 [600/11291 (5%)]\tLoss: 2.319781, Accu: 13.33%\n",
      "Train Epoch: 3 [900/11291 (8%)]\tLoss: 2.309496, Accu: 3.33%\n",
      "Train Epoch: 3 [1200/11291 (11%)]\tLoss: 2.291887, Accu: 16.67%\n",
      "Train Epoch: 3 [1500/11291 (13%)]\tLoss: 2.265055, Accu: 10.00%\n",
      "Train Epoch: 3 [1800/11291 (16%)]\tLoss: 2.266342, Accu: 20.00%\n",
      "Train Epoch: 3 [2100/11291 (19%)]\tLoss: 2.306191, Accu: 13.33%\n",
      "Train Epoch: 3 [2400/11291 (21%)]\tLoss: 2.291072, Accu: 6.67%\n",
      "Train Epoch: 3 [2700/11291 (24%)]\tLoss: 2.308936, Accu: 6.67%\n",
      "Train Epoch: 3 [3000/11291 (27%)]\tLoss: 2.274956, Accu: 10.00%\n",
      "Train Epoch: 3 [3300/11291 (29%)]\tLoss: 2.285957, Accu: 0.00%\n",
      "Train Epoch: 3 [3600/11291 (32%)]\tLoss: 2.327134, Accu: 6.67%\n",
      "Train Epoch: 3 [3900/11291 (34%)]\tLoss: 2.306673, Accu: 13.33%\n",
      "Train Epoch: 3 [4200/11291 (37%)]\tLoss: 2.305179, Accu: 10.00%\n",
      "Train Epoch: 3 [4500/11291 (40%)]\tLoss: 2.304621, Accu: 13.33%\n",
      "Train Epoch: 3 [4800/11291 (42%)]\tLoss: 2.315221, Accu: 6.67%\n",
      "Train Epoch: 3 [5100/11291 (45%)]\tLoss: 2.308814, Accu: 16.67%\n",
      "Train Epoch: 3 [5400/11291 (48%)]\tLoss: 2.317215, Accu: 13.33%\n",
      "Train Epoch: 3 [5700/11291 (50%)]\tLoss: 2.282561, Accu: 20.00%\n",
      "Train Epoch: 3 [6000/11291 (53%)]\tLoss: 2.299249, Accu: 13.33%\n",
      "Train Epoch: 3 [6300/11291 (56%)]\tLoss: 2.304174, Accu: 13.33%\n",
      "Train Epoch: 3 [6600/11291 (58%)]\tLoss: 2.280996, Accu: 23.33%\n",
      "Train Epoch: 3 [6900/11291 (61%)]\tLoss: 2.310015, Accu: 6.67%\n",
      "Train Epoch: 3 [7200/11291 (64%)]\tLoss: 2.267043, Accu: 16.67%\n",
      "Train Epoch: 3 [7500/11291 (66%)]\tLoss: 2.270764, Accu: 16.67%\n",
      "Train Epoch: 3 [7800/11291 (69%)]\tLoss: 2.287608, Accu: 20.00%\n",
      "Train Epoch: 3 [8100/11291 (72%)]\tLoss: 2.319866, Accu: 13.33%\n",
      "Train Epoch: 3 [8400/11291 (74%)]\tLoss: 2.296715, Accu: 13.33%\n",
      "Train Epoch: 3 [8700/11291 (77%)]\tLoss: 2.308974, Accu: 13.33%\n",
      "Train Epoch: 3 [9000/11291 (80%)]\tLoss: 2.306060, Accu: 13.33%\n",
      "Train Epoch: 3 [9300/11291 (82%)]\tLoss: 2.278838, Accu: 23.33%\n",
      "Train Epoch: 3 [9600/11291 (85%)]\tLoss: 2.308910, Accu: 10.00%\n",
      "Train Epoch: 3 [9900/11291 (88%)]\tLoss: 2.286689, Accu: 10.00%\n",
      "Train Epoch: 3 [10200/11291 (90%)]\tLoss: 2.326441, Accu: 6.67%\n",
      "Train Epoch: 3 [10500/11291 (93%)]\tLoss: 2.290647, Accu: 6.67%\n",
      "Train Epoch: 3 [10800/11291 (95%)]\tLoss: 2.304154, Accu: 20.00%\n",
      "Train Epoch: 3 [11100/11291 (98%)]\tLoss: 2.281841, Accu: 16.67%\n",
      "\n",
      "Test set (1411 samples): Average loss: 2.2946, Accuracy: 12.33%\n",
      "\n",
      "Epoch 3 model saved!\n",
      "Train Epoch: 4 [300/11291 (3%)]\tLoss: 2.276054, Accu: 20.00%\n",
      "Train Epoch: 4 [600/11291 (5%)]\tLoss: 2.270390, Accu: 23.33%\n",
      "Train Epoch: 4 [900/11291 (8%)]\tLoss: 2.349117, Accu: 6.67%\n",
      "Train Epoch: 4 [1200/11291 (11%)]\tLoss: 2.320640, Accu: 3.33%\n",
      "Train Epoch: 4 [1500/11291 (13%)]\tLoss: 2.299495, Accu: 10.00%\n",
      "Train Epoch: 4 [1800/11291 (16%)]\tLoss: 2.281408, Accu: 20.00%\n",
      "Train Epoch: 4 [2100/11291 (19%)]\tLoss: 2.302186, Accu: 10.00%\n",
      "Train Epoch: 4 [2400/11291 (21%)]\tLoss: 2.176911, Accu: 26.67%\n",
      "Train Epoch: 4 [2700/11291 (24%)]\tLoss: 2.270838, Accu: 20.00%\n",
      "Train Epoch: 4 [3000/11291 (27%)]\tLoss: 2.269912, Accu: 13.33%\n",
      "Train Epoch: 4 [3300/11291 (29%)]\tLoss: 2.340393, Accu: 6.67%\n",
      "Train Epoch: 4 [3600/11291 (32%)]\tLoss: 2.246516, Accu: 13.33%\n",
      "Train Epoch: 4 [3900/11291 (34%)]\tLoss: 2.260656, Accu: 16.67%\n",
      "Train Epoch: 4 [4200/11291 (37%)]\tLoss: 2.310740, Accu: 10.00%\n",
      "Train Epoch: 4 [4500/11291 (40%)]\tLoss: 2.273135, Accu: 3.33%\n",
      "Train Epoch: 4 [4800/11291 (42%)]\tLoss: 2.267238, Accu: 16.67%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [5100/11291 (45%)]\tLoss: 2.238179, Accu: 20.00%\n",
      "Train Epoch: 4 [5400/11291 (48%)]\tLoss: 2.346227, Accu: 13.33%\n",
      "Train Epoch: 4 [5700/11291 (50%)]\tLoss: 2.259326, Accu: 20.00%\n",
      "Train Epoch: 4 [6000/11291 (53%)]\tLoss: 2.386608, Accu: 6.67%\n",
      "Train Epoch: 4 [6300/11291 (56%)]\tLoss: 2.290549, Accu: 10.00%\n",
      "Train Epoch: 4 [6600/11291 (58%)]\tLoss: 2.217415, Accu: 33.33%\n",
      "Train Epoch: 4 [6900/11291 (61%)]\tLoss: 2.234910, Accu: 20.00%\n",
      "Train Epoch: 4 [7200/11291 (64%)]\tLoss: 2.297312, Accu: 13.33%\n",
      "Train Epoch: 4 [7500/11291 (66%)]\tLoss: 2.242831, Accu: 10.00%\n",
      "Train Epoch: 4 [7800/11291 (69%)]\tLoss: 2.226393, Accu: 26.67%\n",
      "Train Epoch: 4 [8100/11291 (72%)]\tLoss: 2.269879, Accu: 6.67%\n",
      "Train Epoch: 4 [8400/11291 (74%)]\tLoss: 2.219188, Accu: 23.33%\n",
      "Train Epoch: 4 [8700/11291 (77%)]\tLoss: 2.294271, Accu: 10.00%\n",
      "Train Epoch: 4 [9000/11291 (80%)]\tLoss: 2.227433, Accu: 20.00%\n",
      "Train Epoch: 4 [9300/11291 (82%)]\tLoss: 2.279032, Accu: 13.33%\n",
      "Train Epoch: 4 [9600/11291 (85%)]\tLoss: 2.231400, Accu: 16.67%\n",
      "Train Epoch: 4 [9900/11291 (88%)]\tLoss: 2.290452, Accu: 10.00%\n",
      "Train Epoch: 4 [10200/11291 (90%)]\tLoss: 2.293804, Accu: 10.00%\n",
      "Train Epoch: 4 [10500/11291 (93%)]\tLoss: 2.260407, Accu: 16.67%\n",
      "Train Epoch: 4 [10800/11291 (95%)]\tLoss: 2.246296, Accu: 23.33%\n",
      "Train Epoch: 4 [11100/11291 (98%)]\tLoss: 2.209673, Accu: 23.33%\n",
      "\n",
      "Test set (1411 samples): Average loss: 2.2473, Accuracy: 16.80%\n",
      "\n",
      "Epoch 4 model saved!\n",
      "Train Epoch: 5 [300/11291 (3%)]\tLoss: 2.331203, Accu: 20.00%\n",
      "Train Epoch: 5 [600/11291 (5%)]\tLoss: 2.182399, Accu: 13.33%\n",
      "Train Epoch: 5 [900/11291 (8%)]\tLoss: 2.270377, Accu: 16.67%\n",
      "Train Epoch: 5 [1200/11291 (11%)]\tLoss: 2.158946, Accu: 26.67%\n",
      "Train Epoch: 5 [1500/11291 (13%)]\tLoss: 2.289804, Accu: 20.00%\n",
      "Train Epoch: 5 [1800/11291 (16%)]\tLoss: 2.106771, Accu: 33.33%\n",
      "Train Epoch: 5 [2100/11291 (19%)]\tLoss: 2.184387, Accu: 23.33%\n",
      "Train Epoch: 5 [2400/11291 (21%)]\tLoss: 2.142675, Accu: 26.67%\n",
      "Train Epoch: 5 [2700/11291 (24%)]\tLoss: 2.183181, Accu: 20.00%\n",
      "Train Epoch: 5 [3000/11291 (27%)]\tLoss: 2.199940, Accu: 23.33%\n",
      "Train Epoch: 5 [3300/11291 (29%)]\tLoss: 2.206006, Accu: 20.00%\n",
      "Train Epoch: 5 [3600/11291 (32%)]\tLoss: 2.182479, Accu: 13.33%\n",
      "Train Epoch: 5 [3900/11291 (34%)]\tLoss: 2.252227, Accu: 26.67%\n",
      "Train Epoch: 5 [4200/11291 (37%)]\tLoss: 2.284573, Accu: 16.67%\n",
      "Train Epoch: 5 [4500/11291 (40%)]\tLoss: 2.180975, Accu: 16.67%\n",
      "Train Epoch: 5 [4800/11291 (42%)]\tLoss: 2.158494, Accu: 13.33%\n",
      "Train Epoch: 5 [5100/11291 (45%)]\tLoss: 2.106000, Accu: 30.00%\n",
      "Train Epoch: 5 [5400/11291 (48%)]\tLoss: 2.194335, Accu: 6.67%\n",
      "Train Epoch: 5 [5700/11291 (50%)]\tLoss: 2.260274, Accu: 26.67%\n",
      "Train Epoch: 5 [6000/11291 (53%)]\tLoss: 2.133897, Accu: 23.33%\n",
      "Train Epoch: 5 [6300/11291 (56%)]\tLoss: 2.247845, Accu: 20.00%\n",
      "Train Epoch: 5 [6600/11291 (58%)]\tLoss: 2.068438, Accu: 30.00%\n",
      "Train Epoch: 5 [6900/11291 (61%)]\tLoss: 2.272688, Accu: 6.67%\n",
      "Train Epoch: 5 [7200/11291 (64%)]\tLoss: 2.255234, Accu: 13.33%\n",
      "Train Epoch: 5 [7500/11291 (66%)]\tLoss: 2.141586, Accu: 26.67%\n",
      "Train Epoch: 5 [7800/11291 (69%)]\tLoss: 2.330346, Accu: 13.33%\n",
      "Train Epoch: 5 [8100/11291 (72%)]\tLoss: 2.360806, Accu: 6.67%\n",
      "Train Epoch: 5 [8400/11291 (74%)]\tLoss: 2.181002, Accu: 20.00%\n",
      "Train Epoch: 5 [8700/11291 (77%)]\tLoss: 2.108324, Accu: 23.33%\n",
      "Train Epoch: 5 [9000/11291 (80%)]\tLoss: 2.218519, Accu: 13.33%\n",
      "Train Epoch: 5 [9300/11291 (82%)]\tLoss: 2.127098, Accu: 26.67%\n",
      "Train Epoch: 5 [9600/11291 (85%)]\tLoss: 2.088305, Accu: 26.67%\n",
      "Train Epoch: 5 [9900/11291 (88%)]\tLoss: 2.064855, Accu: 23.33%\n",
      "Train Epoch: 5 [10200/11291 (90%)]\tLoss: 2.223146, Accu: 13.33%\n",
      "Train Epoch: 5 [10500/11291 (93%)]\tLoss: 2.211217, Accu: 26.67%\n",
      "Train Epoch: 5 [10800/11291 (95%)]\tLoss: 2.225566, Accu: 13.33%\n",
      "Train Epoch: 5 [11100/11291 (98%)]\tLoss: 2.072480, Accu: 20.00%\n",
      "\n",
      "Test set (1411 samples): Average loss: 2.2120, Accuracy: 19.49%\n",
      "\n",
      "Epoch 5 model saved!\n",
      "Train Epoch: 6 [300/11291 (3%)]\tLoss: 2.112891, Accu: 23.33%\n",
      "Train Epoch: 6 [600/11291 (5%)]\tLoss: 2.150011, Accu: 33.33%\n",
      "Train Epoch: 6 [900/11291 (8%)]\tLoss: 2.204381, Accu: 16.67%\n",
      "Train Epoch: 6 [1200/11291 (11%)]\tLoss: 2.253343, Accu: 3.33%\n",
      "Train Epoch: 6 [1500/11291 (13%)]\tLoss: 2.129868, Accu: 30.00%\n",
      "Train Epoch: 6 [1800/11291 (16%)]\tLoss: 2.353501, Accu: 16.67%\n",
      "Train Epoch: 6 [2100/11291 (19%)]\tLoss: 2.186291, Accu: 20.00%\n",
      "Train Epoch: 6 [2400/11291 (21%)]\tLoss: 2.185157, Accu: 23.33%\n",
      "Train Epoch: 6 [2700/11291 (24%)]\tLoss: 2.158675, Accu: 30.00%\n",
      "Train Epoch: 6 [3000/11291 (27%)]\tLoss: 2.145840, Accu: 20.00%\n",
      "Train Epoch: 6 [3300/11291 (29%)]\tLoss: 2.329602, Accu: 16.67%\n",
      "Train Epoch: 6 [3600/11291 (32%)]\tLoss: 2.269565, Accu: 16.67%\n",
      "Train Epoch: 6 [3900/11291 (34%)]\tLoss: 2.048156, Accu: 33.33%\n",
      "Train Epoch: 6 [4200/11291 (37%)]\tLoss: 2.267116, Accu: 13.33%\n",
      "Train Epoch: 6 [4500/11291 (40%)]\tLoss: 2.066080, Accu: 26.67%\n",
      "Train Epoch: 6 [4800/11291 (42%)]\tLoss: 1.952825, Accu: 33.33%\n",
      "Train Epoch: 6 [5100/11291 (45%)]\tLoss: 1.997843, Accu: 30.00%\n",
      "Train Epoch: 6 [5400/11291 (48%)]\tLoss: 2.369089, Accu: 10.00%\n",
      "Train Epoch: 6 [5700/11291 (50%)]\tLoss: 2.191468, Accu: 20.00%\n",
      "Train Epoch: 6 [6000/11291 (53%)]\tLoss: 2.038220, Accu: 30.00%\n",
      "Train Epoch: 6 [6300/11291 (56%)]\tLoss: 2.035068, Accu: 30.00%\n",
      "Train Epoch: 6 [6600/11291 (58%)]\tLoss: 2.085845, Accu: 23.33%\n",
      "Train Epoch: 6 [6900/11291 (61%)]\tLoss: 2.021962, Accu: 30.00%\n",
      "Train Epoch: 6 [7200/11291 (64%)]\tLoss: 2.296983, Accu: 16.67%\n",
      "Train Epoch: 6 [7500/11291 (66%)]\tLoss: 2.163764, Accu: 30.00%\n",
      "Train Epoch: 6 [7800/11291 (69%)]\tLoss: 2.031452, Accu: 30.00%\n",
      "Train Epoch: 6 [8100/11291 (72%)]\tLoss: 2.249638, Accu: 20.00%\n",
      "Train Epoch: 6 [8400/11291 (74%)]\tLoss: 1.985825, Accu: 30.00%\n",
      "Train Epoch: 6 [8700/11291 (77%)]\tLoss: 2.145762, Accu: 13.33%\n",
      "Train Epoch: 6 [9000/11291 (80%)]\tLoss: 2.069046, Accu: 26.67%\n",
      "Train Epoch: 6 [9300/11291 (82%)]\tLoss: 2.054643, Accu: 26.67%\n",
      "Train Epoch: 6 [9600/11291 (85%)]\tLoss: 2.185304, Accu: 20.00%\n",
      "Train Epoch: 6 [9900/11291 (88%)]\tLoss: 2.040019, Accu: 26.67%\n",
      "Train Epoch: 6 [10200/11291 (90%)]\tLoss: 2.009497, Accu: 33.33%\n",
      "Train Epoch: 6 [10500/11291 (93%)]\tLoss: 2.300688, Accu: 6.67%\n",
      "Train Epoch: 6 [10800/11291 (95%)]\tLoss: 2.110152, Accu: 26.67%\n",
      "Train Epoch: 6 [11100/11291 (98%)]\tLoss: 2.231802, Accu: 20.00%\n",
      "\n",
      "Test set (1411 samples): Average loss: 2.1306, Accuracy: 23.67%\n",
      "\n",
      "Epoch 6 model saved!\n",
      "Train Epoch: 7 [300/11291 (3%)]\tLoss: 2.175973, Accu: 20.00%\n",
      "Train Epoch: 7 [600/11291 (5%)]\tLoss: 2.046553, Accu: 23.33%\n",
      "Train Epoch: 7 [900/11291 (8%)]\tLoss: 2.146529, Accu: 23.33%\n",
      "Train Epoch: 7 [1200/11291 (11%)]\tLoss: 2.218476, Accu: 23.33%\n",
      "Train Epoch: 7 [1500/11291 (13%)]\tLoss: 2.001052, Accu: 30.00%\n",
      "Train Epoch: 7 [1800/11291 (16%)]\tLoss: 2.178528, Accu: 26.67%\n",
      "Train Epoch: 7 [2100/11291 (19%)]\tLoss: 1.806568, Accu: 43.33%\n",
      "Train Epoch: 7 [2400/11291 (21%)]\tLoss: 2.243969, Accu: 20.00%\n",
      "Train Epoch: 7 [2700/11291 (24%)]\tLoss: 2.188071, Accu: 20.00%\n",
      "Train Epoch: 7 [3000/11291 (27%)]\tLoss: 2.189247, Accu: 23.33%\n",
      "Train Epoch: 7 [3300/11291 (29%)]\tLoss: 2.114319, Accu: 13.33%\n",
      "Train Epoch: 7 [3600/11291 (32%)]\tLoss: 1.969039, Accu: 36.67%\n",
      "Train Epoch: 7 [3900/11291 (34%)]\tLoss: 2.185081, Accu: 13.33%\n",
      "Train Epoch: 7 [4200/11291 (37%)]\tLoss: 2.278239, Accu: 13.33%\n",
      "Train Epoch: 7 [4500/11291 (40%)]\tLoss: 1.905620, Accu: 36.67%\n",
      "Train Epoch: 7 [4800/11291 (42%)]\tLoss: 2.085656, Accu: 23.33%\n",
      "Train Epoch: 7 [5100/11291 (45%)]\tLoss: 2.186338, Accu: 33.33%\n",
      "Train Epoch: 7 [5400/11291 (48%)]\tLoss: 2.074208, Accu: 20.00%\n",
      "Train Epoch: 7 [5700/11291 (50%)]\tLoss: 1.915805, Accu: 33.33%\n",
      "Train Epoch: 7 [6000/11291 (53%)]\tLoss: 2.149340, Accu: 23.33%\n",
      "Train Epoch: 7 [6300/11291 (56%)]\tLoss: 2.201546, Accu: 33.33%\n",
      "Train Epoch: 7 [6600/11291 (58%)]\tLoss: 2.054473, Accu: 20.00%\n",
      "Train Epoch: 7 [6900/11291 (61%)]\tLoss: 1.995978, Accu: 26.67%\n",
      "Train Epoch: 7 [7200/11291 (64%)]\tLoss: 2.065497, Accu: 33.33%\n",
      "Train Epoch: 7 [7500/11291 (66%)]\tLoss: 2.055890, Accu: 36.67%\n",
      "Train Epoch: 7 [7800/11291 (69%)]\tLoss: 1.925740, Accu: 40.00%\n",
      "Train Epoch: 7 [8100/11291 (72%)]\tLoss: 1.907566, Accu: 30.00%\n",
      "Train Epoch: 7 [8400/11291 (74%)]\tLoss: 2.088424, Accu: 13.33%\n",
      "Train Epoch: 7 [8700/11291 (77%)]\tLoss: 2.010827, Accu: 30.00%\n",
      "Train Epoch: 7 [9000/11291 (80%)]\tLoss: 2.115267, Accu: 30.00%\n",
      "Train Epoch: 7 [9300/11291 (82%)]\tLoss: 2.017377, Accu: 23.33%\n",
      "Train Epoch: 7 [9600/11291 (85%)]\tLoss: 2.052132, Accu: 23.33%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [9900/11291 (88%)]\tLoss: 2.186033, Accu: 30.00%\n",
      "Train Epoch: 7 [10200/11291 (90%)]\tLoss: 1.952615, Accu: 30.00%\n",
      "Train Epoch: 7 [10500/11291 (93%)]\tLoss: 2.103530, Accu: 23.33%\n",
      "Train Epoch: 7 [10800/11291 (95%)]\tLoss: 1.998035, Accu: 23.33%\n",
      "Train Epoch: 7 [11100/11291 (98%)]\tLoss: 1.975627, Accu: 30.00%\n",
      "\n",
      "Test set (1411 samples): Average loss: 2.1095, Accuracy: 24.24%\n",
      "\n",
      "Epoch 7 model saved!\n",
      "Train Epoch: 8 [300/11291 (3%)]\tLoss: 1.999081, Accu: 26.67%\n",
      "Train Epoch: 8 [600/11291 (5%)]\tLoss: 1.983601, Accu: 33.33%\n",
      "Train Epoch: 8 [900/11291 (8%)]\tLoss: 1.885367, Accu: 33.33%\n",
      "Train Epoch: 8 [1200/11291 (11%)]\tLoss: 1.997138, Accu: 23.33%\n",
      "Train Epoch: 8 [1500/11291 (13%)]\tLoss: 1.861692, Accu: 43.33%\n",
      "Train Epoch: 8 [1800/11291 (16%)]\tLoss: 2.015401, Accu: 30.00%\n",
      "Train Epoch: 8 [2100/11291 (19%)]\tLoss: 2.211111, Accu: 20.00%\n",
      "Train Epoch: 8 [2400/11291 (21%)]\tLoss: 1.955205, Accu: 36.67%\n",
      "Train Epoch: 8 [2700/11291 (24%)]\tLoss: 2.234811, Accu: 20.00%\n",
      "Train Epoch: 8 [3000/11291 (27%)]\tLoss: 1.989918, Accu: 30.00%\n",
      "Train Epoch: 8 [3300/11291 (29%)]\tLoss: 1.989960, Accu: 30.00%\n",
      "Train Epoch: 8 [3600/11291 (32%)]\tLoss: 1.783886, Accu: 36.67%\n",
      "Train Epoch: 8 [3900/11291 (34%)]\tLoss: 1.952581, Accu: 20.00%\n",
      "Train Epoch: 8 [4200/11291 (37%)]\tLoss: 2.180707, Accu: 20.00%\n",
      "Train Epoch: 8 [4500/11291 (40%)]\tLoss: 1.771162, Accu: 36.67%\n",
      "Train Epoch: 8 [4800/11291 (42%)]\tLoss: 1.990604, Accu: 36.67%\n",
      "Train Epoch: 8 [5100/11291 (45%)]\tLoss: 2.183958, Accu: 23.33%\n",
      "Train Epoch: 8 [5400/11291 (48%)]\tLoss: 2.256688, Accu: 20.00%\n",
      "Train Epoch: 8 [5700/11291 (50%)]\tLoss: 2.137032, Accu: 26.67%\n",
      "Train Epoch: 8 [6000/11291 (53%)]\tLoss: 2.230936, Accu: 23.33%\n",
      "Train Epoch: 8 [6300/11291 (56%)]\tLoss: 2.187584, Accu: 16.67%\n",
      "Train Epoch: 8 [6600/11291 (58%)]\tLoss: 1.965055, Accu: 23.33%\n",
      "Train Epoch: 8 [6900/11291 (61%)]\tLoss: 2.086477, Accu: 16.67%\n",
      "Train Epoch: 8 [7200/11291 (64%)]\tLoss: 2.104813, Accu: 23.33%\n",
      "Train Epoch: 8 [7500/11291 (66%)]\tLoss: 2.109119, Accu: 26.67%\n",
      "Train Epoch: 8 [7800/11291 (69%)]\tLoss: 1.903184, Accu: 33.33%\n",
      "Train Epoch: 8 [8100/11291 (72%)]\tLoss: 2.095585, Accu: 20.00%\n",
      "Train Epoch: 8 [8400/11291 (74%)]\tLoss: 1.800815, Accu: 33.33%\n",
      "Train Epoch: 8 [8700/11291 (77%)]\tLoss: 1.790579, Accu: 30.00%\n",
      "Train Epoch: 8 [9000/11291 (80%)]\tLoss: 1.934749, Accu: 33.33%\n",
      "Train Epoch: 8 [9300/11291 (82%)]\tLoss: 1.868269, Accu: 30.00%\n",
      "Train Epoch: 8 [9600/11291 (85%)]\tLoss: 2.108701, Accu: 13.33%\n",
      "Train Epoch: 8 [9900/11291 (88%)]\tLoss: 1.960896, Accu: 33.33%\n",
      "Train Epoch: 8 [10200/11291 (90%)]\tLoss: 2.224644, Accu: 23.33%\n",
      "Train Epoch: 8 [10500/11291 (93%)]\tLoss: 2.096420, Accu: 26.67%\n",
      "Train Epoch: 8 [10800/11291 (95%)]\tLoss: 2.028440, Accu: 30.00%\n",
      "Train Epoch: 8 [11100/11291 (98%)]\tLoss: 2.047128, Accu: 26.67%\n",
      "\n",
      "Test set (1411 samples): Average loss: 2.0889, Accuracy: 25.37%\n",
      "\n",
      "Epoch 8 model saved!\n",
      "Train Epoch: 9 [300/11291 (3%)]\tLoss: 1.916706, Accu: 26.67%\n",
      "Train Epoch: 9 [600/11291 (5%)]\tLoss: 1.599200, Accu: 50.00%\n",
      "Train Epoch: 9 [900/11291 (8%)]\tLoss: 1.914928, Accu: 36.67%\n",
      "Train Epoch: 9 [1200/11291 (11%)]\tLoss: 1.778420, Accu: 30.00%\n",
      "Train Epoch: 9 [1500/11291 (13%)]\tLoss: 2.027660, Accu: 33.33%\n",
      "Train Epoch: 9 [1800/11291 (16%)]\tLoss: 1.875419, Accu: 43.33%\n",
      "Train Epoch: 9 [2100/11291 (19%)]\tLoss: 1.675826, Accu: 36.67%\n",
      "Train Epoch: 9 [2400/11291 (21%)]\tLoss: 2.219693, Accu: 20.00%\n",
      "Train Epoch: 9 [2700/11291 (24%)]\tLoss: 2.009568, Accu: 26.67%\n",
      "Train Epoch: 9 [3000/11291 (27%)]\tLoss: 1.697720, Accu: 40.00%\n",
      "Train Epoch: 9 [3300/11291 (29%)]\tLoss: 2.053342, Accu: 16.67%\n",
      "Train Epoch: 9 [3600/11291 (32%)]\tLoss: 1.795891, Accu: 43.33%\n",
      "Train Epoch: 9 [3900/11291 (34%)]\tLoss: 1.902841, Accu: 36.67%\n",
      "Train Epoch: 9 [4200/11291 (37%)]\tLoss: 2.102507, Accu: 20.00%\n",
      "Train Epoch: 9 [4500/11291 (40%)]\tLoss: 1.984009, Accu: 30.00%\n",
      "Train Epoch: 9 [4800/11291 (42%)]\tLoss: 1.875255, Accu: 26.67%\n",
      "Train Epoch: 9 [5100/11291 (45%)]\tLoss: 2.069356, Accu: 26.67%\n",
      "Train Epoch: 9 [5400/11291 (48%)]\tLoss: 1.536534, Accu: 46.67%\n",
      "Train Epoch: 9 [5700/11291 (50%)]\tLoss: 2.025638, Accu: 26.67%\n",
      "Train Epoch: 9 [6000/11291 (53%)]\tLoss: 1.884978, Accu: 30.00%\n",
      "Train Epoch: 9 [6300/11291 (56%)]\tLoss: 1.836694, Accu: 26.67%\n",
      "Train Epoch: 9 [6600/11291 (58%)]\tLoss: 1.988313, Accu: 43.33%\n",
      "Train Epoch: 9 [6900/11291 (61%)]\tLoss: 2.003297, Accu: 33.33%\n",
      "Train Epoch: 9 [7200/11291 (64%)]\tLoss: 1.742378, Accu: 36.67%\n",
      "Train Epoch: 9 [7500/11291 (66%)]\tLoss: 1.918866, Accu: 23.33%\n",
      "Train Epoch: 9 [7800/11291 (69%)]\tLoss: 1.916262, Accu: 20.00%\n",
      "Train Epoch: 9 [8100/11291 (72%)]\tLoss: 1.839329, Accu: 40.00%\n",
      "Train Epoch: 9 [8400/11291 (74%)]\tLoss: 1.785768, Accu: 33.33%\n",
      "Train Epoch: 9 [8700/11291 (77%)]\tLoss: 1.927603, Accu: 30.00%\n",
      "Train Epoch: 9 [9000/11291 (80%)]\tLoss: 1.827339, Accu: 36.67%\n",
      "Train Epoch: 9 [9300/11291 (82%)]\tLoss: 1.973537, Accu: 23.33%\n",
      "Train Epoch: 9 [9600/11291 (85%)]\tLoss: 1.968326, Accu: 30.00%\n",
      "Train Epoch: 9 [9900/11291 (88%)]\tLoss: 2.047863, Accu: 23.33%\n",
      "Train Epoch: 9 [10200/11291 (90%)]\tLoss: 1.735134, Accu: 50.00%\n",
      "Train Epoch: 9 [10500/11291 (93%)]\tLoss: 2.181127, Accu: 23.33%\n",
      "Train Epoch: 9 [10800/11291 (95%)]\tLoss: 2.038916, Accu: 33.33%\n",
      "Train Epoch: 9 [11100/11291 (98%)]\tLoss: 1.808700, Accu: 33.33%\n",
      "\n",
      "Test set (1411 samples): Average loss: 2.0560, Accuracy: 25.80%\n",
      "\n",
      "Epoch 9 model saved!\n",
      "Train Epoch: 10 [300/11291 (3%)]\tLoss: 1.806459, Accu: 36.67%\n",
      "Train Epoch: 10 [600/11291 (5%)]\tLoss: 1.966809, Accu: 23.33%\n",
      "Train Epoch: 10 [900/11291 (8%)]\tLoss: 1.674980, Accu: 36.67%\n",
      "Train Epoch: 10 [1200/11291 (11%)]\tLoss: 1.601032, Accu: 36.67%\n",
      "Train Epoch: 10 [1500/11291 (13%)]\tLoss: 2.063056, Accu: 26.67%\n",
      "Train Epoch: 10 [1800/11291 (16%)]\tLoss: 1.810545, Accu: 26.67%\n",
      "Train Epoch: 10 [2100/11291 (19%)]\tLoss: 2.004344, Accu: 36.67%\n",
      "Train Epoch: 10 [2400/11291 (21%)]\tLoss: 1.906892, Accu: 30.00%\n",
      "Train Epoch: 10 [2700/11291 (24%)]\tLoss: 1.981106, Accu: 30.00%\n",
      "Train Epoch: 10 [3000/11291 (27%)]\tLoss: 2.040528, Accu: 33.33%\n",
      "Train Epoch: 10 [3300/11291 (29%)]\tLoss: 1.982290, Accu: 33.33%\n",
      "Train Epoch: 10 [3600/11291 (32%)]\tLoss: 1.762995, Accu: 36.67%\n",
      "Train Epoch: 10 [3900/11291 (34%)]\tLoss: 1.825369, Accu: 36.67%\n",
      "Train Epoch: 10 [4200/11291 (37%)]\tLoss: 1.625434, Accu: 36.67%\n",
      "Train Epoch: 10 [4500/11291 (40%)]\tLoss: 1.982841, Accu: 16.67%\n",
      "Train Epoch: 10 [4800/11291 (42%)]\tLoss: 1.979880, Accu: 26.67%\n",
      "Train Epoch: 10 [5100/11291 (45%)]\tLoss: 1.832506, Accu: 33.33%\n",
      "Train Epoch: 10 [5400/11291 (48%)]\tLoss: 1.904125, Accu: 30.00%\n",
      "Train Epoch: 10 [5700/11291 (50%)]\tLoss: 1.853138, Accu: 33.33%\n",
      "Train Epoch: 10 [6000/11291 (53%)]\tLoss: 2.103467, Accu: 26.67%\n",
      "Train Epoch: 10 [6300/11291 (56%)]\tLoss: 1.948251, Accu: 20.00%\n",
      "Train Epoch: 10 [6600/11291 (58%)]\tLoss: 1.998957, Accu: 26.67%\n",
      "Train Epoch: 10 [6900/11291 (61%)]\tLoss: 1.981118, Accu: 20.00%\n",
      "Train Epoch: 10 [7200/11291 (64%)]\tLoss: 1.754543, Accu: 36.67%\n",
      "Train Epoch: 10 [7500/11291 (66%)]\tLoss: 1.979141, Accu: 30.00%\n",
      "Train Epoch: 10 [7800/11291 (69%)]\tLoss: 1.850729, Accu: 30.00%\n",
      "Train Epoch: 10 [8100/11291 (72%)]\tLoss: 1.913743, Accu: 40.00%\n",
      "Train Epoch: 10 [8400/11291 (74%)]\tLoss: 1.936478, Accu: 30.00%\n",
      "Train Epoch: 10 [8700/11291 (77%)]\tLoss: 1.582737, Accu: 46.67%\n",
      "Train Epoch: 10 [9000/11291 (80%)]\tLoss: 1.929643, Accu: 26.67%\n",
      "Train Epoch: 10 [9300/11291 (82%)]\tLoss: 1.849422, Accu: 36.67%\n",
      "Train Epoch: 10 [9600/11291 (85%)]\tLoss: 1.822034, Accu: 30.00%\n",
      "Train Epoch: 10 [9900/11291 (88%)]\tLoss: 1.545446, Accu: 50.00%\n",
      "Train Epoch: 10 [10200/11291 (90%)]\tLoss: 1.756007, Accu: 36.67%\n",
      "Train Epoch: 10 [10500/11291 (93%)]\tLoss: 1.591446, Accu: 36.67%\n",
      "Train Epoch: 10 [10800/11291 (95%)]\tLoss: 2.014761, Accu: 23.33%\n",
      "Train Epoch: 10 [11100/11291 (98%)]\tLoss: 1.896394, Accu: 36.67%\n",
      "\n",
      "Test set (1411 samples): Average loss: 2.0678, Accuracy: 28.35%\n",
      "\n",
      "Epoch 10 model saved!\n",
      "Train Epoch: 11 [300/11291 (3%)]\tLoss: 1.500056, Accu: 60.00%\n",
      "Train Epoch: 11 [600/11291 (5%)]\tLoss: 1.967375, Accu: 30.00%\n",
      "Train Epoch: 11 [900/11291 (8%)]\tLoss: 1.449656, Accu: 53.33%\n",
      "Train Epoch: 11 [1200/11291 (11%)]\tLoss: 1.751757, Accu: 33.33%\n",
      "Train Epoch: 11 [1500/11291 (13%)]\tLoss: 1.544268, Accu: 50.00%\n",
      "Train Epoch: 11 [1800/11291 (16%)]\tLoss: 1.771398, Accu: 40.00%\n",
      "Train Epoch: 11 [2100/11291 (19%)]\tLoss: 2.091201, Accu: 23.33%\n",
      "Train Epoch: 11 [2400/11291 (21%)]\tLoss: 1.903064, Accu: 30.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11 [2700/11291 (24%)]\tLoss: 1.670300, Accu: 40.00%\n",
      "Train Epoch: 11 [3000/11291 (27%)]\tLoss: 1.597772, Accu: 46.67%\n",
      "Train Epoch: 11 [3300/11291 (29%)]\tLoss: 1.634016, Accu: 40.00%\n",
      "Train Epoch: 11 [3600/11291 (32%)]\tLoss: 1.595057, Accu: 53.33%\n",
      "Train Epoch: 11 [3900/11291 (34%)]\tLoss: 1.973713, Accu: 26.67%\n",
      "Train Epoch: 11 [4200/11291 (37%)]\tLoss: 1.646212, Accu: 46.67%\n",
      "Train Epoch: 11 [4500/11291 (40%)]\tLoss: 1.973778, Accu: 23.33%\n",
      "Train Epoch: 11 [4800/11291 (42%)]\tLoss: 1.498554, Accu: 43.33%\n",
      "Train Epoch: 11 [5100/11291 (45%)]\tLoss: 1.754393, Accu: 36.67%\n",
      "Train Epoch: 11 [5400/11291 (48%)]\tLoss: 1.795821, Accu: 30.00%\n",
      "Train Epoch: 11 [5700/11291 (50%)]\tLoss: 1.655784, Accu: 36.67%\n",
      "Train Epoch: 11 [6000/11291 (53%)]\tLoss: 1.905223, Accu: 33.33%\n",
      "Train Epoch: 11 [6300/11291 (56%)]\tLoss: 2.196051, Accu: 20.00%\n",
      "Train Epoch: 11 [6600/11291 (58%)]\tLoss: 1.878654, Accu: 33.33%\n",
      "Train Epoch: 11 [6900/11291 (61%)]\tLoss: 1.864751, Accu: 43.33%\n",
      "Train Epoch: 11 [7200/11291 (64%)]\tLoss: 1.873573, Accu: 40.00%\n",
      "Train Epoch: 11 [7500/11291 (66%)]\tLoss: 1.897952, Accu: 36.67%\n",
      "Train Epoch: 11 [7800/11291 (69%)]\tLoss: 1.440127, Accu: 50.00%\n",
      "Train Epoch: 11 [8100/11291 (72%)]\tLoss: 1.823288, Accu: 26.67%\n",
      "Train Epoch: 11 [8400/11291 (74%)]\tLoss: 1.773499, Accu: 26.67%\n",
      "Train Epoch: 11 [8700/11291 (77%)]\tLoss: 1.714915, Accu: 43.33%\n",
      "Train Epoch: 11 [9000/11291 (80%)]\tLoss: 1.781533, Accu: 40.00%\n",
      "Train Epoch: 11 [9300/11291 (82%)]\tLoss: 1.916260, Accu: 36.67%\n",
      "Train Epoch: 11 [9600/11291 (85%)]\tLoss: 1.647912, Accu: 40.00%\n",
      "Train Epoch: 11 [9900/11291 (88%)]\tLoss: 1.854063, Accu: 43.33%\n",
      "Train Epoch: 11 [10200/11291 (90%)]\tLoss: 1.735607, Accu: 46.67%\n",
      "Train Epoch: 11 [10500/11291 (93%)]\tLoss: 1.603914, Accu: 40.00%\n",
      "Train Epoch: 11 [10800/11291 (95%)]\tLoss: 1.689260, Accu: 33.33%\n",
      "Train Epoch: 11 [11100/11291 (98%)]\tLoss: 1.675278, Accu: 43.33%\n",
      "\n",
      "Test set (1411 samples): Average loss: 2.0859, Accuracy: 27.99%\n",
      "\n",
      "Epoch 11 model saved!\n",
      "Train Epoch: 12 [300/11291 (3%)]\tLoss: 1.540512, Accu: 36.67%\n",
      "Train Epoch: 12 [600/11291 (5%)]\tLoss: 1.786062, Accu: 43.33%\n",
      "Train Epoch: 12 [900/11291 (8%)]\tLoss: 1.745043, Accu: 46.67%\n",
      "Train Epoch: 12 [1200/11291 (11%)]\tLoss: 1.753594, Accu: 36.67%\n",
      "Train Epoch: 12 [1500/11291 (13%)]\tLoss: 1.537725, Accu: 40.00%\n",
      "Train Epoch: 12 [1800/11291 (16%)]\tLoss: 1.542375, Accu: 46.67%\n",
      "Train Epoch: 12 [2100/11291 (19%)]\tLoss: 1.801386, Accu: 33.33%\n",
      "Train Epoch: 12 [2400/11291 (21%)]\tLoss: 1.644072, Accu: 43.33%\n",
      "Train Epoch: 12 [2700/11291 (24%)]\tLoss: 1.672426, Accu: 43.33%\n",
      "Train Epoch: 12 [3000/11291 (27%)]\tLoss: 1.807005, Accu: 33.33%\n",
      "Train Epoch: 12 [3300/11291 (29%)]\tLoss: 1.423842, Accu: 50.00%\n",
      "Train Epoch: 12 [3600/11291 (32%)]\tLoss: 2.089506, Accu: 33.33%\n",
      "Train Epoch: 12 [3900/11291 (34%)]\tLoss: 1.965346, Accu: 30.00%\n",
      "Train Epoch: 12 [4200/11291 (37%)]\tLoss: 1.468296, Accu: 60.00%\n",
      "Train Epoch: 12 [4500/11291 (40%)]\tLoss: 1.835866, Accu: 43.33%\n",
      "Train Epoch: 12 [4800/11291 (42%)]\tLoss: 1.548427, Accu: 46.67%\n",
      "Train Epoch: 12 [5100/11291 (45%)]\tLoss: 1.630558, Accu: 40.00%\n",
      "Train Epoch: 12 [5400/11291 (48%)]\tLoss: 1.792210, Accu: 26.67%\n",
      "Train Epoch: 12 [5700/11291 (50%)]\tLoss: 1.560523, Accu: 43.33%\n",
      "Train Epoch: 12 [6000/11291 (53%)]\tLoss: 1.740049, Accu: 40.00%\n",
      "Train Epoch: 12 [6300/11291 (56%)]\tLoss: 1.641891, Accu: 46.67%\n",
      "Train Epoch: 12 [6600/11291 (58%)]\tLoss: 1.578577, Accu: 43.33%\n",
      "Train Epoch: 12 [6900/11291 (61%)]\tLoss: 1.686709, Accu: 50.00%\n",
      "Train Epoch: 12 [7200/11291 (64%)]\tLoss: 1.794737, Accu: 33.33%\n",
      "Train Epoch: 12 [7500/11291 (66%)]\tLoss: 1.394415, Accu: 63.33%\n",
      "Train Epoch: 12 [7800/11291 (69%)]\tLoss: 2.091677, Accu: 40.00%\n",
      "Train Epoch: 12 [8100/11291 (72%)]\tLoss: 1.519330, Accu: 50.00%\n",
      "Train Epoch: 12 [8400/11291 (74%)]\tLoss: 1.654281, Accu: 40.00%\n",
      "Train Epoch: 12 [8700/11291 (77%)]\tLoss: 1.575909, Accu: 50.00%\n",
      "Train Epoch: 12 [9000/11291 (80%)]\tLoss: 1.468961, Accu: 46.67%\n",
      "Train Epoch: 12 [9300/11291 (82%)]\tLoss: 1.572125, Accu: 53.33%\n",
      "Train Epoch: 12 [9600/11291 (85%)]\tLoss: 1.394570, Accu: 53.33%\n",
      "Train Epoch: 12 [9900/11291 (88%)]\tLoss: 1.861929, Accu: 33.33%\n",
      "Train Epoch: 12 [10200/11291 (90%)]\tLoss: 1.790272, Accu: 30.00%\n",
      "Train Epoch: 12 [10500/11291 (93%)]\tLoss: 1.625535, Accu: 46.67%\n",
      "Train Epoch: 12 [10800/11291 (95%)]\tLoss: 1.500513, Accu: 50.00%\n",
      "Train Epoch: 12 [11100/11291 (98%)]\tLoss: 1.790965, Accu: 30.00%\n",
      "\n",
      "Test set (1411 samples): Average loss: 2.1170, Accuracy: 26.15%\n",
      "\n",
      "Epoch 12 model saved!\n",
      "Train Epoch: 13 [300/11291 (3%)]\tLoss: 1.430561, Accu: 43.33%\n",
      "Train Epoch: 13 [600/11291 (5%)]\tLoss: 1.556266, Accu: 43.33%\n",
      "Train Epoch: 13 [900/11291 (8%)]\tLoss: 1.457396, Accu: 63.33%\n",
      "Train Epoch: 13 [1200/11291 (11%)]\tLoss: 1.519575, Accu: 56.67%\n",
      "Train Epoch: 13 [1500/11291 (13%)]\tLoss: 1.777547, Accu: 40.00%\n",
      "Train Epoch: 13 [1800/11291 (16%)]\tLoss: 1.884662, Accu: 30.00%\n",
      "Train Epoch: 13 [2100/11291 (19%)]\tLoss: 1.689047, Accu: 40.00%\n",
      "Train Epoch: 13 [2400/11291 (21%)]\tLoss: 1.457792, Accu: 36.67%\n",
      "Train Epoch: 13 [2700/11291 (24%)]\tLoss: 1.903352, Accu: 40.00%\n",
      "Train Epoch: 13 [3000/11291 (27%)]\tLoss: 1.460984, Accu: 50.00%\n",
      "Train Epoch: 13 [3300/11291 (29%)]\tLoss: 1.534791, Accu: 43.33%\n",
      "Train Epoch: 13 [3600/11291 (32%)]\tLoss: 1.614056, Accu: 40.00%\n",
      "Train Epoch: 13 [3900/11291 (34%)]\tLoss: 1.740354, Accu: 43.33%\n",
      "Train Epoch: 13 [4200/11291 (37%)]\tLoss: 1.624216, Accu: 40.00%\n",
      "Train Epoch: 13 [4500/11291 (40%)]\tLoss: 1.567066, Accu: 46.67%\n",
      "Train Epoch: 13 [4800/11291 (42%)]\tLoss: 1.989432, Accu: 43.33%\n",
      "Train Epoch: 13 [5100/11291 (45%)]\tLoss: 1.151471, Accu: 50.00%\n",
      "Train Epoch: 13 [5400/11291 (48%)]\tLoss: 1.285863, Accu: 53.33%\n",
      "Train Epoch: 13 [5700/11291 (50%)]\tLoss: 1.625846, Accu: 33.33%\n",
      "Train Epoch: 13 [6000/11291 (53%)]\tLoss: 1.670282, Accu: 40.00%\n",
      "Train Epoch: 13 [6300/11291 (56%)]\tLoss: 1.369085, Accu: 43.33%\n",
      "Train Epoch: 13 [6600/11291 (58%)]\tLoss: 1.405585, Accu: 50.00%\n",
      "Train Epoch: 13 [6900/11291 (61%)]\tLoss: 1.728413, Accu: 40.00%\n",
      "Train Epoch: 13 [7200/11291 (64%)]\tLoss: 1.666155, Accu: 46.67%\n",
      "Train Epoch: 13 [7500/11291 (66%)]\tLoss: 1.718652, Accu: 33.33%\n",
      "Train Epoch: 13 [7800/11291 (69%)]\tLoss: 1.557615, Accu: 46.67%\n",
      "Train Epoch: 13 [8100/11291 (72%)]\tLoss: 1.819683, Accu: 33.33%\n",
      "Train Epoch: 13 [8400/11291 (74%)]\tLoss: 1.620921, Accu: 46.67%\n",
      "Train Epoch: 13 [8700/11291 (77%)]\tLoss: 1.803110, Accu: 36.67%\n",
      "Train Epoch: 13 [9000/11291 (80%)]\tLoss: 1.601992, Accu: 40.00%\n",
      "Train Epoch: 13 [9300/11291 (82%)]\tLoss: 1.378552, Accu: 56.67%\n",
      "Train Epoch: 13 [9600/11291 (85%)]\tLoss: 1.664938, Accu: 46.67%\n",
      "Train Epoch: 13 [9900/11291 (88%)]\tLoss: 1.887772, Accu: 33.33%\n",
      "Train Epoch: 13 [10200/11291 (90%)]\tLoss: 1.341742, Accu: 56.67%\n",
      "Train Epoch: 13 [10500/11291 (93%)]\tLoss: 1.663321, Accu: 53.33%\n",
      "Train Epoch: 13 [10800/11291 (95%)]\tLoss: 1.568161, Accu: 40.00%\n",
      "Train Epoch: 13 [11100/11291 (98%)]\tLoss: 1.462193, Accu: 46.67%\n",
      "\n",
      "Test set (1411 samples): Average loss: 2.1982, Accuracy: 26.44%\n",
      "\n",
      "Epoch 13 model saved!\n",
      "Train Epoch: 14 [300/11291 (3%)]\tLoss: 1.498112, Accu: 50.00%\n",
      "Train Epoch: 14 [600/11291 (5%)]\tLoss: 1.150333, Accu: 70.00%\n",
      "Train Epoch: 14 [900/11291 (8%)]\tLoss: 1.672634, Accu: 43.33%\n",
      "Train Epoch: 14 [1200/11291 (11%)]\tLoss: 1.449809, Accu: 50.00%\n",
      "Train Epoch: 14 [1500/11291 (13%)]\tLoss: 1.032737, Accu: 66.67%\n",
      "Train Epoch: 14 [1800/11291 (16%)]\tLoss: 1.833202, Accu: 30.00%\n",
      "Train Epoch: 14 [2100/11291 (19%)]\tLoss: 1.360714, Accu: 53.33%\n",
      "Train Epoch: 14 [2400/11291 (21%)]\tLoss: 1.389197, Accu: 50.00%\n",
      "Train Epoch: 14 [2700/11291 (24%)]\tLoss: 1.537581, Accu: 46.67%\n",
      "Train Epoch: 14 [3000/11291 (27%)]\tLoss: 1.579506, Accu: 40.00%\n",
      "Train Epoch: 14 [3300/11291 (29%)]\tLoss: 1.877732, Accu: 23.33%\n",
      "Train Epoch: 14 [3600/11291 (32%)]\tLoss: 1.424099, Accu: 43.33%\n",
      "Train Epoch: 14 [3900/11291 (34%)]\tLoss: 1.544663, Accu: 43.33%\n",
      "Train Epoch: 14 [4200/11291 (37%)]\tLoss: 1.815002, Accu: 36.67%\n",
      "Train Epoch: 14 [4500/11291 (40%)]\tLoss: 1.608226, Accu: 40.00%\n",
      "Train Epoch: 14 [4800/11291 (42%)]\tLoss: 1.569151, Accu: 53.33%\n",
      "Train Epoch: 14 [5100/11291 (45%)]\tLoss: 1.407015, Accu: 43.33%\n",
      "Train Epoch: 14 [5400/11291 (48%)]\tLoss: 1.344680, Accu: 53.33%\n",
      "Train Epoch: 14 [5700/11291 (50%)]\tLoss: 1.717018, Accu: 20.00%\n",
      "Train Epoch: 14 [6000/11291 (53%)]\tLoss: 1.302404, Accu: 53.33%\n",
      "Train Epoch: 14 [6300/11291 (56%)]\tLoss: 1.670200, Accu: 50.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14 [6600/11291 (58%)]\tLoss: 1.618941, Accu: 43.33%\n",
      "Train Epoch: 14 [6900/11291 (61%)]\tLoss: 1.849803, Accu: 30.00%\n",
      "Train Epoch: 14 [7200/11291 (64%)]\tLoss: 1.519549, Accu: 50.00%\n",
      "Train Epoch: 14 [7500/11291 (66%)]\tLoss: 1.426838, Accu: 50.00%\n",
      "Train Epoch: 14 [7800/11291 (69%)]\tLoss: 1.504105, Accu: 43.33%\n",
      "Train Epoch: 14 [8100/11291 (72%)]\tLoss: 1.861728, Accu: 36.67%\n",
      "Train Epoch: 14 [8400/11291 (74%)]\tLoss: 1.244117, Accu: 50.00%\n",
      "Train Epoch: 14 [8700/11291 (77%)]\tLoss: 1.518197, Accu: 40.00%\n",
      "Train Epoch: 14 [9000/11291 (80%)]\tLoss: 1.307816, Accu: 56.67%\n",
      "Train Epoch: 14 [9300/11291 (82%)]\tLoss: 1.318134, Accu: 63.33%\n",
      "Train Epoch: 14 [9600/11291 (85%)]\tLoss: 1.384173, Accu: 46.67%\n",
      "Train Epoch: 14 [9900/11291 (88%)]\tLoss: 1.414358, Accu: 46.67%\n",
      "Train Epoch: 14 [10200/11291 (90%)]\tLoss: 1.502051, Accu: 50.00%\n",
      "Train Epoch: 14 [10500/11291 (93%)]\tLoss: 1.579587, Accu: 26.67%\n",
      "Train Epoch: 14 [10800/11291 (95%)]\tLoss: 1.487026, Accu: 43.33%\n",
      "Train Epoch: 14 [11100/11291 (98%)]\tLoss: 1.499243, Accu: 46.67%\n",
      "\n",
      "Test set (1411 samples): Average loss: 2.2443, Accuracy: 25.30%\n",
      "\n",
      "Epoch 14 model saved!\n",
      "Train Epoch: 15 [300/11291 (3%)]\tLoss: 1.226627, Accu: 50.00%\n",
      "Train Epoch: 15 [600/11291 (5%)]\tLoss: 1.237229, Accu: 63.33%\n",
      "Train Epoch: 15 [900/11291 (8%)]\tLoss: 1.238060, Accu: 60.00%\n",
      "Train Epoch: 15 [1200/11291 (11%)]\tLoss: 1.151837, Accu: 50.00%\n",
      "Train Epoch: 15 [1500/11291 (13%)]\tLoss: 1.355217, Accu: 56.67%\n",
      "Train Epoch: 15 [1800/11291 (16%)]\tLoss: 1.266251, Accu: 43.33%\n",
      "Train Epoch: 15 [2100/11291 (19%)]\tLoss: 1.134581, Accu: 56.67%\n",
      "Train Epoch: 15 [2400/11291 (21%)]\tLoss: 1.302603, Accu: 53.33%\n",
      "Train Epoch: 15 [2700/11291 (24%)]\tLoss: 1.487222, Accu: 46.67%\n",
      "Train Epoch: 15 [3000/11291 (27%)]\tLoss: 1.525294, Accu: 56.67%\n",
      "Train Epoch: 15 [3300/11291 (29%)]\tLoss: 1.528364, Accu: 46.67%\n",
      "Train Epoch: 15 [3600/11291 (32%)]\tLoss: 1.380157, Accu: 56.67%\n",
      "Train Epoch: 15 [3900/11291 (34%)]\tLoss: 1.472174, Accu: 53.33%\n",
      "Train Epoch: 15 [4200/11291 (37%)]\tLoss: 1.589088, Accu: 46.67%\n",
      "Train Epoch: 15 [4500/11291 (40%)]\tLoss: 1.526473, Accu: 53.33%\n",
      "Train Epoch: 15 [4800/11291 (42%)]\tLoss: 1.334287, Accu: 56.67%\n",
      "Train Epoch: 15 [5100/11291 (45%)]\tLoss: 1.791663, Accu: 46.67%\n",
      "Train Epoch: 15 [5400/11291 (48%)]\tLoss: 1.306860, Accu: 60.00%\n",
      "Train Epoch: 15 [5700/11291 (50%)]\tLoss: 1.186639, Accu: 66.67%\n",
      "Train Epoch: 15 [6000/11291 (53%)]\tLoss: 1.469288, Accu: 46.67%\n",
      "Train Epoch: 15 [6300/11291 (56%)]\tLoss: 1.534629, Accu: 46.67%\n",
      "Train Epoch: 15 [6600/11291 (58%)]\tLoss: 1.469436, Accu: 50.00%\n",
      "Train Epoch: 15 [6900/11291 (61%)]\tLoss: 1.197930, Accu: 73.33%\n",
      "Train Epoch: 15 [7200/11291 (64%)]\tLoss: 1.577924, Accu: 36.67%\n",
      "Train Epoch: 15 [7500/11291 (66%)]\tLoss: 1.479639, Accu: 43.33%\n",
      "Train Epoch: 15 [7800/11291 (69%)]\tLoss: 1.572611, Accu: 43.33%\n",
      "Train Epoch: 15 [8100/11291 (72%)]\tLoss: 1.511693, Accu: 46.67%\n",
      "Train Epoch: 15 [8400/11291 (74%)]\tLoss: 1.063325, Accu: 66.67%\n",
      "Train Epoch: 15 [8700/11291 (77%)]\tLoss: 1.249505, Accu: 53.33%\n",
      "Train Epoch: 15 [9000/11291 (80%)]\tLoss: 1.507843, Accu: 43.33%\n",
      "Train Epoch: 15 [9300/11291 (82%)]\tLoss: 1.517767, Accu: 56.67%\n",
      "Train Epoch: 15 [9600/11291 (85%)]\tLoss: 1.314670, Accu: 40.00%\n",
      "Train Epoch: 15 [9900/11291 (88%)]\tLoss: 1.366132, Accu: 66.67%\n",
      "Train Epoch: 15 [10200/11291 (90%)]\tLoss: 1.427178, Accu: 36.67%\n",
      "Train Epoch: 15 [10500/11291 (93%)]\tLoss: 1.559400, Accu: 43.33%\n",
      "Train Epoch: 15 [10800/11291 (95%)]\tLoss: 1.654146, Accu: 40.00%\n",
      "Train Epoch: 15 [11100/11291 (98%)]\tLoss: 1.370198, Accu: 60.00%\n",
      "\n",
      "Test set (1411 samples): Average loss: 2.2816, Accuracy: 28.21%\n",
      "\n",
      "Epoch 15 model saved!\n",
      "Train Epoch: 16 [300/11291 (3%)]\tLoss: 1.117799, Accu: 60.00%\n",
      "Train Epoch: 16 [600/11291 (5%)]\tLoss: 1.190407, Accu: 56.67%\n",
      "Train Epoch: 16 [900/11291 (8%)]\tLoss: 1.245374, Accu: 60.00%\n",
      "Train Epoch: 16 [1200/11291 (11%)]\tLoss: 1.300256, Accu: 53.33%\n",
      "Train Epoch: 16 [1500/11291 (13%)]\tLoss: 1.118223, Accu: 53.33%\n",
      "Train Epoch: 16 [1800/11291 (16%)]\tLoss: 1.126385, Accu: 63.33%\n",
      "Train Epoch: 16 [2100/11291 (19%)]\tLoss: 1.359998, Accu: 46.67%\n",
      "Train Epoch: 16 [2400/11291 (21%)]\tLoss: 0.995614, Accu: 56.67%\n",
      "Train Epoch: 16 [2700/11291 (24%)]\tLoss: 1.407725, Accu: 53.33%\n",
      "Train Epoch: 16 [3000/11291 (27%)]\tLoss: 1.173324, Accu: 53.33%\n",
      "Train Epoch: 16 [3300/11291 (29%)]\tLoss: 1.255457, Accu: 50.00%\n",
      "Train Epoch: 16 [3600/11291 (32%)]\tLoss: 1.208135, Accu: 50.00%\n",
      "Train Epoch: 16 [3900/11291 (34%)]\tLoss: 1.220499, Accu: 56.67%\n",
      "Train Epoch: 16 [4200/11291 (37%)]\tLoss: 1.130762, Accu: 60.00%\n",
      "Train Epoch: 16 [4500/11291 (40%)]\tLoss: 1.071901, Accu: 56.67%\n",
      "Train Epoch: 16 [4800/11291 (42%)]\tLoss: 1.214029, Accu: 63.33%\n",
      "Train Epoch: 16 [5100/11291 (45%)]\tLoss: 1.435585, Accu: 40.00%\n",
      "Train Epoch: 16 [5400/11291 (48%)]\tLoss: 1.216522, Accu: 66.67%\n",
      "Train Epoch: 16 [5700/11291 (50%)]\tLoss: 1.183904, Accu: 53.33%\n",
      "Train Epoch: 16 [6000/11291 (53%)]\tLoss: 1.795871, Accu: 46.67%\n",
      "Train Epoch: 16 [6300/11291 (56%)]\tLoss: 1.127991, Accu: 63.33%\n",
      "Train Epoch: 16 [6600/11291 (58%)]\tLoss: 1.090332, Accu: 63.33%\n",
      "Train Epoch: 16 [6900/11291 (61%)]\tLoss: 1.616413, Accu: 43.33%\n",
      "Train Epoch: 16 [7200/11291 (64%)]\tLoss: 1.185444, Accu: 63.33%\n",
      "Train Epoch: 16 [7500/11291 (66%)]\tLoss: 1.485464, Accu: 50.00%\n",
      "Train Epoch: 16 [7800/11291 (69%)]\tLoss: 1.255946, Accu: 60.00%\n",
      "Train Epoch: 16 [8100/11291 (72%)]\tLoss: 1.090749, Accu: 70.00%\n",
      "Train Epoch: 16 [8400/11291 (74%)]\tLoss: 0.918673, Accu: 70.00%\n",
      "Train Epoch: 16 [8700/11291 (77%)]\tLoss: 1.083576, Accu: 66.67%\n",
      "Train Epoch: 16 [9000/11291 (80%)]\tLoss: 1.494326, Accu: 43.33%\n",
      "Train Epoch: 16 [9300/11291 (82%)]\tLoss: 1.362602, Accu: 60.00%\n",
      "Train Epoch: 16 [9600/11291 (85%)]\tLoss: 1.334111, Accu: 56.67%\n",
      "Train Epoch: 16 [9900/11291 (88%)]\tLoss: 1.138646, Accu: 63.33%\n",
      "Train Epoch: 16 [10200/11291 (90%)]\tLoss: 1.505880, Accu: 53.33%\n",
      "Train Epoch: 16 [10500/11291 (93%)]\tLoss: 1.250111, Accu: 63.33%\n",
      "Train Epoch: 16 [10800/11291 (95%)]\tLoss: 1.410619, Accu: 53.33%\n",
      "Train Epoch: 16 [11100/11291 (98%)]\tLoss: 1.726960, Accu: 40.00%\n",
      "\n",
      "Test set (1411 samples): Average loss: 2.4759, Accuracy: 25.94%\n",
      "\n",
      "Epoch 16 model saved!\n",
      "Train Epoch: 17 [300/11291 (3%)]\tLoss: 1.179900, Accu: 43.33%\n",
      "Train Epoch: 17 [600/11291 (5%)]\tLoss: 1.030646, Accu: 63.33%\n",
      "Train Epoch: 17 [900/11291 (8%)]\tLoss: 1.068917, Accu: 53.33%\n",
      "Train Epoch: 17 [1200/11291 (11%)]\tLoss: 1.204691, Accu: 53.33%\n",
      "Train Epoch: 17 [1500/11291 (13%)]\tLoss: 1.164481, Accu: 56.67%\n",
      "Train Epoch: 17 [1800/11291 (16%)]\tLoss: 0.905881, Accu: 70.00%\n",
      "Train Epoch: 17 [2100/11291 (19%)]\tLoss: 1.176814, Accu: 56.67%\n",
      "Train Epoch: 17 [2400/11291 (21%)]\tLoss: 1.153080, Accu: 50.00%\n",
      "Train Epoch: 17 [2700/11291 (24%)]\tLoss: 1.183281, Accu: 60.00%\n",
      "Train Epoch: 17 [3000/11291 (27%)]\tLoss: 1.410163, Accu: 46.67%\n",
      "Train Epoch: 17 [3300/11291 (29%)]\tLoss: 1.225269, Accu: 46.67%\n",
      "Train Epoch: 17 [3600/11291 (32%)]\tLoss: 1.350426, Accu: 53.33%\n",
      "Train Epoch: 17 [3900/11291 (34%)]\tLoss: 1.119432, Accu: 60.00%\n",
      "Train Epoch: 17 [4200/11291 (37%)]\tLoss: 0.955045, Accu: 70.00%\n",
      "Train Epoch: 17 [4500/11291 (40%)]\tLoss: 1.128136, Accu: 60.00%\n",
      "Train Epoch: 17 [4800/11291 (42%)]\tLoss: 1.285165, Accu: 53.33%\n",
      "Train Epoch: 17 [5100/11291 (45%)]\tLoss: 1.176608, Accu: 53.33%\n",
      "Train Epoch: 17 [5400/11291 (48%)]\tLoss: 1.464546, Accu: 46.67%\n",
      "Train Epoch: 17 [5700/11291 (50%)]\tLoss: 1.280708, Accu: 50.00%\n",
      "Train Epoch: 17 [6000/11291 (53%)]\tLoss: 1.183285, Accu: 63.33%\n",
      "Train Epoch: 17 [6300/11291 (56%)]\tLoss: 1.043027, Accu: 56.67%\n",
      "Train Epoch: 17 [6600/11291 (58%)]\tLoss: 1.008497, Accu: 73.33%\n",
      "Train Epoch: 17 [6900/11291 (61%)]\tLoss: 0.811072, Accu: 76.67%\n",
      "Train Epoch: 17 [7200/11291 (64%)]\tLoss: 0.926607, Accu: 66.67%\n",
      "Train Epoch: 17 [7500/11291 (66%)]\tLoss: 1.054745, Accu: 53.33%\n",
      "Train Epoch: 17 [7800/11291 (69%)]\tLoss: 1.420226, Accu: 46.67%\n",
      "Train Epoch: 17 [8100/11291 (72%)]\tLoss: 1.104627, Accu: 70.00%\n",
      "Train Epoch: 17 [8400/11291 (74%)]\tLoss: 1.227020, Accu: 60.00%\n",
      "Train Epoch: 17 [8700/11291 (77%)]\tLoss: 0.990959, Accu: 70.00%\n",
      "Train Epoch: 17 [9000/11291 (80%)]\tLoss: 1.187314, Accu: 50.00%\n",
      "Train Epoch: 17 [9300/11291 (82%)]\tLoss: 1.466747, Accu: 40.00%\n",
      "Train Epoch: 17 [9600/11291 (85%)]\tLoss: 1.695273, Accu: 53.33%\n",
      "Train Epoch: 17 [9900/11291 (88%)]\tLoss: 1.406841, Accu: 60.00%\n",
      "Train Epoch: 17 [10200/11291 (90%)]\tLoss: 1.165656, Accu: 53.33%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 17 [10500/11291 (93%)]\tLoss: 1.002819, Accu: 63.33%\n",
      "Train Epoch: 17 [10800/11291 (95%)]\tLoss: 0.959736, Accu: 70.00%\n",
      "Train Epoch: 17 [11100/11291 (98%)]\tLoss: 1.178431, Accu: 60.00%\n",
      "\n",
      "Test set (1411 samples): Average loss: 2.4636, Accuracy: 27.85%\n",
      "\n",
      "Epoch 17 model saved!\n",
      "Train Epoch: 18 [300/11291 (3%)]\tLoss: 1.163982, Accu: 53.33%\n",
      "Train Epoch: 18 [600/11291 (5%)]\tLoss: 1.143237, Accu: 60.00%\n",
      "Train Epoch: 18 [900/11291 (8%)]\tLoss: 1.059573, Accu: 66.67%\n",
      "Train Epoch: 18 [1200/11291 (11%)]\tLoss: 1.118328, Accu: 56.67%\n",
      "Train Epoch: 18 [1500/11291 (13%)]\tLoss: 1.222998, Accu: 63.33%\n",
      "Train Epoch: 18 [1800/11291 (16%)]\tLoss: 0.935010, Accu: 73.33%\n",
      "Train Epoch: 18 [2100/11291 (19%)]\tLoss: 0.803184, Accu: 80.00%\n",
      "Train Epoch: 18 [2400/11291 (21%)]\tLoss: 1.057136, Accu: 60.00%\n",
      "Train Epoch: 18 [2700/11291 (24%)]\tLoss: 0.861009, Accu: 73.33%\n",
      "Train Epoch: 18 [3000/11291 (27%)]\tLoss: 1.212163, Accu: 53.33%\n",
      "Train Epoch: 18 [3300/11291 (29%)]\tLoss: 0.985832, Accu: 63.33%\n",
      "Train Epoch: 18 [3600/11291 (32%)]\tLoss: 1.078412, Accu: 66.67%\n",
      "Train Epoch: 18 [3900/11291 (34%)]\tLoss: 0.975880, Accu: 73.33%\n",
      "Train Epoch: 18 [4200/11291 (37%)]\tLoss: 0.771392, Accu: 70.00%\n",
      "Train Epoch: 18 [4500/11291 (40%)]\tLoss: 0.984481, Accu: 66.67%\n",
      "Train Epoch: 18 [4800/11291 (42%)]\tLoss: 1.245756, Accu: 66.67%\n",
      "Train Epoch: 18 [5100/11291 (45%)]\tLoss: 1.346356, Accu: 46.67%\n",
      "Train Epoch: 18 [5400/11291 (48%)]\tLoss: 0.951661, Accu: 66.67%\n",
      "Train Epoch: 18 [5700/11291 (50%)]\tLoss: 1.314867, Accu: 50.00%\n",
      "Train Epoch: 18 [6000/11291 (53%)]\tLoss: 0.816001, Accu: 66.67%\n",
      "Train Epoch: 18 [6300/11291 (56%)]\tLoss: 1.490407, Accu: 50.00%\n",
      "Train Epoch: 18 [6600/11291 (58%)]\tLoss: 1.018490, Accu: 60.00%\n",
      "Train Epoch: 18 [6900/11291 (61%)]\tLoss: 1.020470, Accu: 66.67%\n",
      "Train Epoch: 18 [7200/11291 (64%)]\tLoss: 1.279772, Accu: 56.67%\n",
      "Train Epoch: 18 [7500/11291 (66%)]\tLoss: 1.343589, Accu: 46.67%\n",
      "Train Epoch: 18 [7800/11291 (69%)]\tLoss: 1.452543, Accu: 60.00%\n",
      "Train Epoch: 18 [8100/11291 (72%)]\tLoss: 1.206957, Accu: 60.00%\n",
      "Train Epoch: 18 [8400/11291 (74%)]\tLoss: 1.097586, Accu: 70.00%\n",
      "Train Epoch: 18 [8700/11291 (77%)]\tLoss: 1.404628, Accu: 50.00%\n",
      "Train Epoch: 18 [9000/11291 (80%)]\tLoss: 1.083461, Accu: 56.67%\n",
      "Train Epoch: 18 [9300/11291 (82%)]\tLoss: 1.652062, Accu: 50.00%\n",
      "Train Epoch: 18 [9600/11291 (85%)]\tLoss: 0.973355, Accu: 66.67%\n",
      "Train Epoch: 18 [9900/11291 (88%)]\tLoss: 0.736688, Accu: 66.67%\n",
      "Train Epoch: 18 [10200/11291 (90%)]\tLoss: 1.082013, Accu: 53.33%\n",
      "Train Epoch: 18 [10500/11291 (93%)]\tLoss: 1.288943, Accu: 56.67%\n",
      "Train Epoch: 18 [10800/11291 (95%)]\tLoss: 1.086801, Accu: 60.00%\n",
      "Train Epoch: 18 [11100/11291 (98%)]\tLoss: 1.299485, Accu: 53.33%\n",
      "\n",
      "Test set (1411 samples): Average loss: 2.6645, Accuracy: 25.94%\n",
      "\n",
      "Epoch 18 model saved!\n",
      "Train Epoch: 19 [300/11291 (3%)]\tLoss: 0.724668, Accu: 73.33%\n",
      "Train Epoch: 19 [600/11291 (5%)]\tLoss: 0.824463, Accu: 66.67%\n",
      "Train Epoch: 19 [900/11291 (8%)]\tLoss: 1.107707, Accu: 63.33%\n",
      "Train Epoch: 19 [1200/11291 (11%)]\tLoss: 0.638583, Accu: 76.67%\n",
      "Train Epoch: 19 [1500/11291 (13%)]\tLoss: 1.008527, Accu: 63.33%\n",
      "Train Epoch: 19 [1800/11291 (16%)]\tLoss: 1.377979, Accu: 56.67%\n",
      "Train Epoch: 19 [2100/11291 (19%)]\tLoss: 0.778054, Accu: 63.33%\n",
      "Train Epoch: 19 [2400/11291 (21%)]\tLoss: 0.925723, Accu: 70.00%\n",
      "Train Epoch: 19 [2700/11291 (24%)]\tLoss: 0.722958, Accu: 73.33%\n",
      "Train Epoch: 19 [3000/11291 (27%)]\tLoss: 1.115793, Accu: 53.33%\n",
      "Train Epoch: 19 [3300/11291 (29%)]\tLoss: 0.964231, Accu: 70.00%\n",
      "Train Epoch: 19 [3600/11291 (32%)]\tLoss: 0.980040, Accu: 60.00%\n",
      "Train Epoch: 19 [3900/11291 (34%)]\tLoss: 0.797050, Accu: 66.67%\n",
      "Train Epoch: 19 [4200/11291 (37%)]\tLoss: 1.106028, Accu: 53.33%\n",
      "Train Epoch: 19 [4500/11291 (40%)]\tLoss: 1.075518, Accu: 60.00%\n",
      "Train Epoch: 19 [4800/11291 (42%)]\tLoss: 1.133971, Accu: 66.67%\n",
      "Train Epoch: 19 [5100/11291 (45%)]\tLoss: 0.800306, Accu: 73.33%\n",
      "Train Epoch: 19 [5400/11291 (48%)]\tLoss: 1.114294, Accu: 63.33%\n",
      "Train Epoch: 19 [5700/11291 (50%)]\tLoss: 1.383374, Accu: 46.67%\n",
      "Train Epoch: 19 [6000/11291 (53%)]\tLoss: 1.279825, Accu: 66.67%\n",
      "Train Epoch: 19 [6300/11291 (56%)]\tLoss: 1.086502, Accu: 60.00%\n",
      "Train Epoch: 19 [6600/11291 (58%)]\tLoss: 0.994335, Accu: 66.67%\n",
      "Train Epoch: 19 [6900/11291 (61%)]\tLoss: 1.124145, Accu: 63.33%\n",
      "Train Epoch: 19 [7200/11291 (64%)]\tLoss: 1.018327, Accu: 66.67%\n",
      "Train Epoch: 19 [7500/11291 (66%)]\tLoss: 0.870336, Accu: 66.67%\n",
      "Train Epoch: 19 [7800/11291 (69%)]\tLoss: 1.805049, Accu: 46.67%\n",
      "Train Epoch: 19 [8100/11291 (72%)]\tLoss: 1.124772, Accu: 63.33%\n",
      "Train Epoch: 19 [8400/11291 (74%)]\tLoss: 1.173600, Accu: 63.33%\n",
      "Train Epoch: 19 [8700/11291 (77%)]\tLoss: 1.353048, Accu: 43.33%\n",
      "Train Epoch: 19 [9000/11291 (80%)]\tLoss: 1.175238, Accu: 53.33%\n",
      "Train Epoch: 19 [9300/11291 (82%)]\tLoss: 1.213454, Accu: 56.67%\n",
      "Train Epoch: 19 [9600/11291 (85%)]\tLoss: 0.977237, Accu: 63.33%\n",
      "Train Epoch: 19 [9900/11291 (88%)]\tLoss: 0.751902, Accu: 66.67%\n",
      "Train Epoch: 19 [10200/11291 (90%)]\tLoss: 0.773059, Accu: 76.67%\n",
      "Train Epoch: 19 [10500/11291 (93%)]\tLoss: 1.225741, Accu: 53.33%\n",
      "Train Epoch: 19 [10800/11291 (95%)]\tLoss: 0.936397, Accu: 73.33%\n",
      "Train Epoch: 19 [11100/11291 (98%)]\tLoss: 0.929692, Accu: 70.00%\n",
      "\n",
      "Test set (1411 samples): Average loss: 2.7400, Accuracy: 25.87%\n",
      "\n",
      "Epoch 19 model saved!\n",
      "Train Epoch: 20 [300/11291 (3%)]\tLoss: 1.017236, Accu: 56.67%\n",
      "Train Epoch: 20 [600/11291 (5%)]\tLoss: 0.901521, Accu: 70.00%\n",
      "Train Epoch: 20 [900/11291 (8%)]\tLoss: 0.728319, Accu: 80.00%\n",
      "Train Epoch: 20 [1200/11291 (11%)]\tLoss: 0.980480, Accu: 56.67%\n",
      "Train Epoch: 20 [1500/11291 (13%)]\tLoss: 0.796863, Accu: 63.33%\n",
      "Train Epoch: 20 [1800/11291 (16%)]\tLoss: 0.996475, Accu: 63.33%\n",
      "Train Epoch: 20 [2100/11291 (19%)]\tLoss: 1.095312, Accu: 63.33%\n",
      "Train Epoch: 20 [2400/11291 (21%)]\tLoss: 0.776037, Accu: 80.00%\n",
      "Train Epoch: 20 [2700/11291 (24%)]\tLoss: 1.250605, Accu: 46.67%\n",
      "Train Epoch: 20 [3000/11291 (27%)]\tLoss: 0.838818, Accu: 73.33%\n",
      "Train Epoch: 20 [3300/11291 (29%)]\tLoss: 0.968592, Accu: 66.67%\n",
      "Train Epoch: 20 [3600/11291 (32%)]\tLoss: 0.680887, Accu: 73.33%\n",
      "Train Epoch: 20 [3900/11291 (34%)]\tLoss: 0.818034, Accu: 60.00%\n",
      "Train Epoch: 20 [4200/11291 (37%)]\tLoss: 1.234223, Accu: 70.00%\n",
      "Train Epoch: 20 [4500/11291 (40%)]\tLoss: 0.704637, Accu: 63.33%\n",
      "Train Epoch: 20 [4800/11291 (42%)]\tLoss: 0.806275, Accu: 66.67%\n",
      "Train Epoch: 20 [5100/11291 (45%)]\tLoss: 0.779979, Accu: 63.33%\n",
      "Train Epoch: 20 [5400/11291 (48%)]\tLoss: 0.830147, Accu: 73.33%\n",
      "Train Epoch: 20 [5700/11291 (50%)]\tLoss: 0.612810, Accu: 76.67%\n",
      "Train Epoch: 20 [6000/11291 (53%)]\tLoss: 1.130185, Accu: 60.00%\n",
      "Train Epoch: 20 [6300/11291 (56%)]\tLoss: 0.778981, Accu: 70.00%\n",
      "Train Epoch: 20 [6600/11291 (58%)]\tLoss: 1.024832, Accu: 60.00%\n",
      "Train Epoch: 20 [6900/11291 (61%)]\tLoss: 0.912519, Accu: 66.67%\n",
      "Train Epoch: 20 [7200/11291 (64%)]\tLoss: 1.073675, Accu: 63.33%\n",
      "Train Epoch: 20 [7500/11291 (66%)]\tLoss: 0.873369, Accu: 66.67%\n",
      "Train Epoch: 20 [7800/11291 (69%)]\tLoss: 0.987483, Accu: 66.67%\n",
      "Train Epoch: 20 [8100/11291 (72%)]\tLoss: 0.779054, Accu: 76.67%\n",
      "Train Epoch: 20 [8400/11291 (74%)]\tLoss: 1.074951, Accu: 50.00%\n",
      "Train Epoch: 20 [8700/11291 (77%)]\tLoss: 0.883502, Accu: 66.67%\n",
      "Train Epoch: 20 [9000/11291 (80%)]\tLoss: 0.974205, Accu: 70.00%\n",
      "Train Epoch: 20 [9300/11291 (82%)]\tLoss: 0.690760, Accu: 76.67%\n",
      "Train Epoch: 20 [9600/11291 (85%)]\tLoss: 0.704625, Accu: 73.33%\n",
      "Train Epoch: 20 [9900/11291 (88%)]\tLoss: 0.678587, Accu: 80.00%\n",
      "Train Epoch: 20 [10200/11291 (90%)]\tLoss: 0.680086, Accu: 70.00%\n",
      "Train Epoch: 20 [10500/11291 (93%)]\tLoss: 1.040524, Accu: 73.33%\n",
      "Train Epoch: 20 [10800/11291 (95%)]\tLoss: 1.061520, Accu: 60.00%\n",
      "Train Epoch: 20 [11100/11291 (98%)]\tLoss: 0.768330, Accu: 76.67%\n",
      "\n",
      "Test set (1411 samples): Average loss: 2.7900, Accuracy: 26.58%\n",
      "\n",
      "Epoch 20 model saved!\n",
      "Train Epoch: 21 [300/11291 (3%)]\tLoss: 1.302626, Accu: 63.33%\n",
      "Train Epoch: 21 [600/11291 (5%)]\tLoss: 1.111733, Accu: 66.67%\n",
      "Train Epoch: 21 [900/11291 (8%)]\tLoss: 1.089561, Accu: 56.67%\n",
      "Train Epoch: 21 [1200/11291 (11%)]\tLoss: 0.977820, Accu: 63.33%\n",
      "Train Epoch: 21 [1500/11291 (13%)]\tLoss: 0.990272, Accu: 63.33%\n",
      "Train Epoch: 21 [1800/11291 (16%)]\tLoss: 0.800063, Accu: 73.33%\n",
      "Train Epoch: 21 [2100/11291 (19%)]\tLoss: 0.997492, Accu: 73.33%\n",
      "Train Epoch: 21 [2400/11291 (21%)]\tLoss: 1.132403, Accu: 60.00%\n",
      "Train Epoch: 21 [2700/11291 (24%)]\tLoss: 0.961874, Accu: 73.33%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 21 [3000/11291 (27%)]\tLoss: 0.990507, Accu: 56.67%\n",
      "Train Epoch: 21 [3300/11291 (29%)]\tLoss: 0.981368, Accu: 76.67%\n",
      "Train Epoch: 21 [3600/11291 (32%)]\tLoss: 0.865385, Accu: 66.67%\n",
      "Train Epoch: 21 [3900/11291 (34%)]\tLoss: 0.788811, Accu: 70.00%\n",
      "Train Epoch: 21 [4200/11291 (37%)]\tLoss: 0.787010, Accu: 73.33%\n",
      "Train Epoch: 21 [4500/11291 (40%)]\tLoss: 0.893853, Accu: 70.00%\n",
      "Train Epoch: 21 [4800/11291 (42%)]\tLoss: 0.988241, Accu: 56.67%\n",
      "Train Epoch: 21 [5100/11291 (45%)]\tLoss: 0.639623, Accu: 76.67%\n",
      "Train Epoch: 21 [5400/11291 (48%)]\tLoss: 1.236294, Accu: 43.33%\n",
      "Train Epoch: 21 [5700/11291 (50%)]\tLoss: 0.885597, Accu: 70.00%\n",
      "Train Epoch: 21 [6000/11291 (53%)]\tLoss: 0.782795, Accu: 76.67%\n",
      "Train Epoch: 21 [6300/11291 (56%)]\tLoss: 0.596961, Accu: 80.00%\n",
      "Train Epoch: 21 [6600/11291 (58%)]\tLoss: 0.925026, Accu: 63.33%\n",
      "Train Epoch: 21 [6900/11291 (61%)]\tLoss: 0.901477, Accu: 66.67%\n",
      "Train Epoch: 21 [7200/11291 (64%)]\tLoss: 0.703064, Accu: 73.33%\n",
      "Train Epoch: 21 [7500/11291 (66%)]\tLoss: 0.907910, Accu: 70.00%\n",
      "Train Epoch: 21 [7800/11291 (69%)]\tLoss: 0.658051, Accu: 80.00%\n",
      "Train Epoch: 21 [8100/11291 (72%)]\tLoss: 1.116809, Accu: 56.67%\n",
      "Train Epoch: 21 [8400/11291 (74%)]\tLoss: 0.672399, Accu: 76.67%\n",
      "Train Epoch: 21 [8700/11291 (77%)]\tLoss: 0.929964, Accu: 70.00%\n",
      "Train Epoch: 21 [9000/11291 (80%)]\tLoss: 1.179774, Accu: 66.67%\n",
      "Train Epoch: 21 [9300/11291 (82%)]\tLoss: 1.153027, Accu: 50.00%\n",
      "Train Epoch: 21 [9600/11291 (85%)]\tLoss: 0.731814, Accu: 66.67%\n",
      "Train Epoch: 21 [9900/11291 (88%)]\tLoss: 0.990917, Accu: 73.33%\n",
      "Train Epoch: 21 [10200/11291 (90%)]\tLoss: 1.023658, Accu: 76.67%\n",
      "Train Epoch: 21 [10500/11291 (93%)]\tLoss: 0.620658, Accu: 73.33%\n",
      "Train Epoch: 21 [10800/11291 (95%)]\tLoss: 0.474264, Accu: 86.67%\n",
      "Train Epoch: 21 [11100/11291 (98%)]\tLoss: 0.665551, Accu: 70.00%\n",
      "\n",
      "Test set (1411 samples): Average loss: 2.9535, Accuracy: 24.73%\n",
      "\n",
      "Epoch 21 model saved!\n",
      "Train Epoch: 22 [300/11291 (3%)]\tLoss: 0.518560, Accu: 90.00%\n",
      "Train Epoch: 22 [600/11291 (5%)]\tLoss: 0.734197, Accu: 76.67%\n",
      "Train Epoch: 22 [900/11291 (8%)]\tLoss: 0.793873, Accu: 76.67%\n",
      "Train Epoch: 22 [1200/11291 (11%)]\tLoss: 0.625101, Accu: 86.67%\n",
      "Train Epoch: 22 [1500/11291 (13%)]\tLoss: 0.676852, Accu: 73.33%\n",
      "Train Epoch: 22 [1800/11291 (16%)]\tLoss: 0.952189, Accu: 76.67%\n",
      "Train Epoch: 22 [2100/11291 (19%)]\tLoss: 0.619337, Accu: 76.67%\n",
      "Train Epoch: 22 [2400/11291 (21%)]\tLoss: 0.727838, Accu: 76.67%\n",
      "Train Epoch: 22 [2700/11291 (24%)]\tLoss: 0.798135, Accu: 70.00%\n",
      "Train Epoch: 22 [3000/11291 (27%)]\tLoss: 0.612638, Accu: 80.00%\n",
      "Train Epoch: 22 [3300/11291 (29%)]\tLoss: 0.630622, Accu: 76.67%\n",
      "Train Epoch: 22 [3600/11291 (32%)]\tLoss: 0.687834, Accu: 73.33%\n",
      "Train Epoch: 22 [3900/11291 (34%)]\tLoss: 0.612278, Accu: 80.00%\n",
      "Train Epoch: 22 [4200/11291 (37%)]\tLoss: 0.577363, Accu: 80.00%\n",
      "Train Epoch: 22 [4500/11291 (40%)]\tLoss: 0.655782, Accu: 70.00%\n",
      "Train Epoch: 22 [4800/11291 (42%)]\tLoss: 0.886140, Accu: 63.33%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pt100]",
   "language": "python",
   "name": "conda-env-.conda-pt100-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
