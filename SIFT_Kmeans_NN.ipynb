{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "from sklearn.decomposition import PCA\n",
    "import ast\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import time\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from tempfile import TemporaryFile\n",
    "from sklearn.cluster import KMeans\n",
    "%matplotlib inline\n",
    "import natsort\n",
    "from sklearn import preprocessing\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SIFT_dataset(path,dataset):\n",
    "    descriptor_list=[]\n",
    "    for video_id in dataset['Index']:\n",
    "        path_train=os.path.join(path, str(video_id))\n",
    "        images=[name for name in os.listdir(path_train) if os.path.isfile(os.path.join(path_train, name))]\n",
    "        images=natsort.natsorted(images)\n",
    "        num_images=len(images)\n",
    "        if num_images>40:\n",
    "            \n",
    "            print(num_images)\n",
    "       \n",
    "        if num_images>25:\n",
    "            images=images[5:25]\n",
    "        if num_images<=25:\n",
    "            images=images[0:20]\n",
    "        for name in images:\n",
    "            file=os.path.join(path_train, str(name))\n",
    "            img_building = cv2.imread(file)\n",
    "            \n",
    "#             plt.figure(figsize=(312,128))\n",
    "#             plt.imshow(img_building)\n",
    "#             plt.show()\n",
    "\n",
    "            img_building = cv2.cvtColor(img_building, cv2.COLOR_BGR2GRAY)   # Convert from cv's BRG default color order to HSV\n",
    "            sift = cv2.xfeatures2d.SIFT_create()\n",
    "            step_size = 15\n",
    "\n",
    "            kp = [cv2.KeyPoint(x, y, step_size) for x in range(0, 312, step_size) for y in range(0, 128, step_size)]\n",
    "            img2 = cv2.drawKeypoints(img_building, kp, None, color=(0,255,0), flags=0)\n",
    "#             plt.figure(figsize=(312,128))\n",
    "#             plt.imshow(img2), plt.show()\n",
    "            \n",
    "            descriptor = sift.compute(img_building, kp)[1]\n",
    "            descriptor_list.append(descriptor)\n",
    "            \n",
    "\n",
    "    return descriptor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(r'/storage/demo_dataset/train_set.csv')\n",
    "test=pd.read_csv(r'/storage/demo_dataset/test_set.csv')\n",
    "val=pd.read_csv(r'/storage/demo_dataset/val_set.csv')\n",
    "\n",
    "path_train ='/storage/demo_dataset/train/'\n",
    "path_test ='/storage/demo_dataset/test/'\n",
    "path_val ='/storage/demo_dataset/val/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "67\n",
      "51\n",
      "41\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b733d9da1b52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#get train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdescriptor_list_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSIFT_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"descriptor_list_train:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptor_list_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# with open(r'/storage/demo_dataset/train_sift.pickle', 'wb') as f:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     pickle.dump(descriptor_list_train, f, protocol=5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "#get train\n",
    "descriptor_list_train=SIFT_dataset(path_train,train)\n",
    "print(\"descriptor_list_train:\",len(descriptor_list_train))\n",
    "with open(r'/storage/demo_dataset/train_sift.pickle', 'wb') as f:\n",
    "    pickle.dump(descriptor_list_train, f, protocol=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "descriptor_list_test: 9840\n"
     ]
    }
   ],
   "source": [
    "#get test\n",
    "descriptor_list_test=SIFT_dataset(path_test,test)\n",
    "print(\"descriptor_list_test:\",len(descriptor_list_test))\n",
    "with open(r'/storage/demo_dataset/test.pickle', 'wb') as f:\n",
    "    pickle.dump(descriptor_list_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "descriptor_list_val: 9860\n"
     ]
    }
   ],
   "source": [
    "#get validation\n",
    "descriptor_list_val=SIFT_dataset(path_val,val)\n",
    "print(\"descriptor_list_val:\",len(descriptor_list_val))\n",
    "\n",
    "with open(r'/storage/demo_dataset/val_sift.pickle', 'wb') as f:\n",
    "    pickle.dump(descriptor_list_val, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "descriptor_list_train: 79000\n",
      "descriptor_list_test: 9840\n",
      "descriptor_list_val: 9860\n"
     ]
    }
   ],
   "source": [
    "with open(r'/storage/demo_dataset/train.pickle', 'rb') as f:\n",
    "    descriptor_list_train = pickle.load(f)\n",
    "print(\"descriptor_list_train:\",len(descriptor_list_train))\n",
    "\n",
    "with open(r'/storage/demo_dataset/test.pickle', 'rb') as f:\n",
    "    descriptor_list_test = pickle.load(f)\n",
    "print(\"descriptor_list_test:\",len(descriptor_list_test))\n",
    "\n",
    "with open(r'/storage/demo_dataset/val_sift.pickle', 'rb') as f:\n",
    "    descriptor_list_val = pickle.load(f)    \n",
    "print(\"descriptor_list_val:\",len(descriptor_list_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_train_desc(descriptor_list):\n",
    "    all_train_desc = []\n",
    "    for i in range(len(descriptor_list)):\n",
    "        for j in range(descriptor_list[i].shape[0]):\n",
    "            all_train_desc.append(descriptor_list[i][j,:])\n",
    "\n",
    "    all_train_desc = np.array(all_train_desc)\n",
    "    return all_train_desc\n",
    "\n",
    "\n",
    "# form training set histograms for each training image using BoW representation\n",
    "def formTrainingSetHistogram(x_train, kmeans, k):\n",
    "    train_hist = []\n",
    "    for i in range(len(x_train)):\n",
    "        data = copy.deepcopy(x_train[i])\n",
    "        predict = kmeans.predict(data)\n",
    "        train_hist.append(np.bincount(predict, minlength=k).reshape(1,-1).ravel())\n",
    "        \n",
    "    return np.array(train_hist)\n",
    "\n",
    "def hist (descriptor_list, kmeans, k):\n",
    "    hist = formTrainingSetHistogram(descriptor_list, kmeans, k)\n",
    "    scaler = preprocessing.StandardScaler().fit(hist)\n",
    "    hist = scaler.transform(hist)\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_desc = all_train_desc(descriptor_list_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=30, random_state=456).fit(all_train_desc)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'/storage/demo_dataset/kmeans.pickle', 'wb') as f:\n",
    "    pickle.dump(km, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'/storage/demo_dataset/kmeans.pickle', 'rb') as f:\n",
    "    kmeans = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form training set histograms for each training image using BoW representation\n",
    "def formTrainingSetHistogram(x_train, kmeans, k):\n",
    "    train_hist = []\n",
    "    for i in range(len(x_train)):\n",
    "        data = copy.deepcopy(x_train[i])\n",
    "        predict = kmeans.predict(data)\n",
    "        train_hist.append(np.bincount(predict, minlength=k).reshape(1,-1).ravel())\n",
    "        \n",
    "    return np.array(train_hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hist = hist(descriptor_list_train, kmeans, 30)\n",
    "test_hist = hist(descriptor_list_test, kmeans, 30)\n",
    "val_hist   = hist(descriptor_list_val, kmeans, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79000, 30)\n",
      "(9840, 30)\n",
      "(9860, 30)\n"
     ]
    }
   ],
   "source": [
    "print(train_hist.shape)\n",
    "print(test_hist.shape)\n",
    "print(val_hist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'/storage/demo_dataset/train_hist.pickle', 'rb') as f:\n",
    "    train_hist = pickle.load(f)\n",
    "\n",
    "with open(r'/storage/demo_dataset/test_hist.pickle', 'rb') as f:\n",
    "    test_hist= pickle.load(f)\n",
    "\n",
    "with open(r'/storage/demo_dataset/val_hist.pickle', 'rb') as f:\n",
    "    val_hist = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change into video-level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_video(hist):\n",
    "    train=[]\n",
    "    for index in range(0,len(hist),20):\n",
    "        result =np.concatenate(hist[index:index+20], axis=0).reshape(20,-1)\n",
    "        train.append(result)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_video=feature_video(train_hist)\n",
    "test_video=feature_video(test_hist)\n",
    "val_video=feature_video(val_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_video: 3950 test_video: 492 val_video: 493\n"
     ]
    }
   ],
   "source": [
    "print('train_video:',len(train_video),'test_video:',len(test_video),'val_video:',len(val_video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 30)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_video[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict={'No gesture':0, \n",
    "            'Pulling Hand In':1, \n",
    "            'Pushing Two Fingers Away':2,\n",
    "            'Rolling Hand Forward':3, \n",
    "            'Shaking Hand':4, \n",
    "            'Sliding Two Fingers Down':5,\n",
    "            'Sliding Two Fingers Left':6, \n",
    "            'Sliding Two Fingers Right':7,\n",
    "            'Thumb Up':8, \n",
    "            'Turning Hand Counterclockwise':9}\n",
    "train_label=np.array(list((map(label_dict.get, train['Label']))))\n",
    "test_label=np.array(list((map(label_dict.get,test['Label']))))\n",
    "val_label=np.array(list((map(label_dict.get,val['Label']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_hist_dataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading FashionMNIST images\"\"\"\n",
    "\n",
    "    def __init__(self, labels, hist,transform):\n",
    "        \n",
    "        self.hist=torch.from_numpy(hist).float()\n",
    "        self.y =torch.from_numpy(labels).float()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "    \n",
    "        label = self.y[index]\n",
    "        hist_per_video=self.hist[index]\n",
    "        return hist_per_video, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Feature_hist_dataset(labels=train_label,hist=np.array(train_video),transform=None)\n",
    "test_dataset = Feature_hist_dataset(labels=test_label,hist=np.array(test_video),transform=None)\n",
    "val_dataset = Feature_hist_dataset(labels=val_label,hist=np.array(val_video),transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "random_seed = 1\n",
    "learning_rate = 0.01\n",
    "num_epochs = 12\n",
    "batch_size = 32\n",
    "\n",
    "# Architecture\n",
    "num_features = 20*30\n",
    "num_hidden_1 = 128\n",
    "num_hidden_2 = 256\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True,\n",
    "                         num_workers=8)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=batch_size, \n",
    "                         shuffle=False,\n",
    "                        num_workers=8)\n",
    "\n",
    "val_loader=DataLoader(dataset=val_dataset, \n",
    "                         batch_size=batch_size, \n",
    "                         shuffle=False,\n",
    "                        num_workers=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### MODEL\n",
    "##########################\n",
    "\n",
    "class MultilayerPerceptron(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        \n",
    "        ### 1st hidden layer\n",
    "        self.linear_1 = torch.nn.Linear(num_features, num_hidden_1)\n",
    "        self.linear_1_bn = torch.nn.BatchNorm1d(num_hidden_1)\n",
    "        torch.nn.Dropout(0.2)\n",
    "        \n",
    "        ### 2nd hidden layer\n",
    "        self.linear_2 = torch.nn.Linear(num_hidden_1, num_hidden_2)\n",
    "        self.linear_2_bn = torch.nn.BatchNorm1d(num_hidden_2)\n",
    "        \n",
    "        ### Output layer\n",
    "        self.linear_out = torch.nn.Linear(num_hidden_2, num_classes)\n",
    "        self.linear_out.weight.detach().normal_(0.0, 0.1)\n",
    "        self.linear_out.bias.detach().zero_()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear_1(x)\n",
    "        # note that batchnorm is in the classic\n",
    "        # sense placed before the activation\n",
    "        out = self.linear_1_bn(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        out = self.linear_2(out)\n",
    "        out = self.linear_2_bn(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        logits = self.linear_out(out)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas\n",
    "\n",
    "    \n",
    "torch.manual_seed(random_seed)\n",
    "model = MultilayerPerceptron(num_features=num_features,\n",
    "                             num_classes=num_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/012 | Batch 000/124 | Cost: 2.6312\n",
      "Epoch: 001/012 | Batch 050/124 | Cost: 2.3747\n",
      "Epoch: 001/012 | Batch 100/124 | Cost: 2.1833\n",
      "Epoch: 001/012 training accuracy: 25.11%\n",
      "Epoch: 001/012 validation accuracy: 14.40%\n",
      "Time elapsed: 0.10 min\n",
      "Epoch: 002/012 | Batch 000/124 | Cost: 2.1160\n",
      "Epoch: 002/012 | Batch 050/124 | Cost: 2.1970\n",
      "Epoch: 002/012 | Batch 100/124 | Cost: 2.0899\n",
      "Epoch: 002/012 training accuracy: 33.44%\n",
      "Epoch: 002/012 validation accuracy: 16.02%\n",
      "Time elapsed: 0.21 min\n",
      "Epoch: 003/012 | Batch 000/124 | Cost: 1.8181\n",
      "Epoch: 003/012 | Batch 050/124 | Cost: 1.9824\n",
      "Epoch: 003/012 | Batch 100/124 | Cost: 1.9607\n",
      "Epoch: 003/012 training accuracy: 37.22%\n",
      "Epoch: 003/012 validation accuracy: 16.84%\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 004/012 | Batch 000/124 | Cost: 1.5802\n",
      "Epoch: 004/012 | Batch 050/124 | Cost: 1.8433\n",
      "Epoch: 004/012 | Batch 100/124 | Cost: 2.0637\n",
      "Epoch: 004/012 training accuracy: 42.66%\n",
      "Epoch: 004/012 validation accuracy: 18.66%\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 005/012 | Batch 000/124 | Cost: 1.8980\n",
      "Epoch: 005/012 | Batch 050/124 | Cost: 1.9439\n",
      "Epoch: 005/012 | Batch 100/124 | Cost: 2.1785\n",
      "Epoch: 005/012 training accuracy: 46.89%\n",
      "Epoch: 005/012 validation accuracy: 17.85%\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 006/012 | Batch 000/124 | Cost: 1.5819\n",
      "Epoch: 006/012 | Batch 050/124 | Cost: 1.6573\n",
      "Epoch: 006/012 | Batch 100/124 | Cost: 1.7940\n",
      "Epoch: 006/012 training accuracy: 50.20%\n",
      "Epoch: 006/012 validation accuracy: 18.66%\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 007/012 | Batch 000/124 | Cost: 1.4110\n",
      "Epoch: 007/012 | Batch 050/124 | Cost: 1.7500\n",
      "Epoch: 007/012 | Batch 100/124 | Cost: 1.6580\n",
      "Epoch: 007/012 training accuracy: 51.32%\n",
      "Epoch: 007/012 validation accuracy: 18.46%\n",
      "Time elapsed: 0.73 min\n",
      "Epoch: 008/012 | Batch 000/124 | Cost: 1.5442\n",
      "Epoch: 008/012 | Batch 050/124 | Cost: 1.5767\n",
      "Epoch: 008/012 | Batch 100/124 | Cost: 1.6707\n",
      "Epoch: 008/012 training accuracy: 53.85%\n",
      "Epoch: 008/012 validation accuracy: 19.07%\n",
      "Time elapsed: 0.83 min\n",
      "Epoch: 009/012 | Batch 000/124 | Cost: 1.3753\n",
      "Epoch: 009/012 | Batch 050/124 | Cost: 1.6544\n",
      "Epoch: 009/012 | Batch 100/124 | Cost: 1.4041\n",
      "Epoch: 009/012 training accuracy: 56.33%\n",
      "Epoch: 009/012 validation accuracy: 18.05%\n",
      "Time elapsed: 0.93 min\n",
      "Epoch: 010/012 | Batch 000/124 | Cost: 1.4053\n",
      "Epoch: 010/012 | Batch 050/124 | Cost: 2.1179\n",
      "Epoch: 010/012 | Batch 100/124 | Cost: 1.5603\n",
      "Epoch: 010/012 training accuracy: 60.58%\n",
      "Epoch: 010/012 validation accuracy: 19.88%\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 011/012 | Batch 000/124 | Cost: 1.3396\n",
      "Epoch: 011/012 | Batch 050/124 | Cost: 1.5584\n",
      "Epoch: 011/012 | Batch 100/124 | Cost: 1.4771\n",
      "Epoch: 011/012 training accuracy: 60.43%\n",
      "Epoch: 011/012 validation accuracy: 17.65%\n",
      "Time elapsed: 1.14 min\n",
      "Epoch: 012/012 | Batch 000/124 | Cost: 1.3667\n",
      "Epoch: 012/012 | Batch 050/124 | Cost: 1.0047\n",
      "Epoch: 012/012 | Batch 100/124 | Cost: 1.6610\n",
      "Epoch: 012/012 training accuracy: 63.11%\n",
      "Epoch: 012/012 validation accuracy: 19.47%\n",
      "Time elapsed: 1.24 min\n",
      "Total Training Time: 1.24 min\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(net, data_loader):\n",
    "    net.eval()\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for features, targets in data_loader:\n",
    "            features = features.view(-1, 20*30).float().to(device)\n",
    "            targets = targets.long().to(device)\n",
    "            logits, probas = net(features)\n",
    "            _, predicted_labels = torch.max(probas, 1)\n",
    "            num_examples += targets.size(0)\n",
    "            correct_pred += (predicted_labels == targets).sum()\n",
    "        return correct_pred.float()/num_examples * 100\n",
    "    \n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.view(-1, 20*30).to(device)\n",
    "        targets = targets.to(device)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits, probas = model(features)\n",
    "        cost = F.cross_entropy(logits, targets.long())\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' %(epoch+1, num_epochs, batch_idx, len(train_loader), cost))\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print('Epoch: %03d/%03d training accuracy: %.2f%%' % (epoch+1, num_epochs, compute_accuracy(model, train_loader)))\n",
    "        print('Epoch: %03d/%03d validation accuracy: %.2f%%' % (epoch+1, num_epochs, compute_accuracy(model, val_loader)))\n",
    "        \n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "    \n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 21.75%\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 22.52%\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: %.2f%%' % (compute_accuracy(model, val_loader)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
